"""
WebSocket STT —Å–µ—Ä–≤–µ—Ä –° —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤
–û–±—ã—á–Ω—ã–π OpenAI Whisper Small + SpeechBrain ECAPA
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –Ω–∞ GPU
"""
import warnings
warnings.filterwarnings("ignore")

# –ü–∞—Ç—á–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–î–û –∏–º–ø–æ—Ä—Ç–∞ speechbrain!)
import os
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

import torchaudio
if not hasattr(torchaudio, 'list_audio_backends'):
    torchaudio.list_audio_backends = lambda: ["sox", "soundfile"]

from huggingface_hub import hf_hub_download as original_hf_hub_download
def patched_hf_hub_download(*args, **kwargs):
    kwargs.pop('use_auth_token', None)
    return original_hf_hub_download(*args, **kwargs)

import huggingface_hub
huggingface_hub.hf_hub_download = patched_hf_hub_download

import speechbrain.utils.fetching as fetching_module
original_link = fetching_module.link_with_strategy

def patched_link_with_strategy(src, dst, strategy="auto"):
    import shutil
    from pathlib import Path
    src_path = Path(src)
    dst_path = Path(dst)
    dst_path.parent.mkdir(parents=True, exist_ok=True)
    if src_path.is_file():
        shutil.copy2(src_path, dst_path)
    return dst_path

fetching_module.link_with_strategy = patched_link_with_strategy

import asyncio
import websockets
import json
import whisper
import torch
import numpy as np
from datetime import datetime
import base64
import time
from speechbrain.inference.speaker import SpeakerRecognition
import pickle
from pathlib import Path

# ============ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô ============
print("=" * 80)
print("üöÄ WEBSOCKET STT + SPEAKER RECOGNITION")
print("=" * 80)

# –§–æ—Ä—Å–∏—Ä—É–µ–º GPU —Ä–µ–∂–∏–º
if not torch.cuda.is_available():
    print("‚ùå CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ CUDA Toolkit –∏ GPU –¥—Ä–∞–π–≤–µ—Ä—ã")
    print("   https://developer.nvidia.com/cuda-downloads")
    import sys
    sys.exit(1)

device = "cuda"
print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º Whisper Small
print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º Whisper Small ({device.upper()})...")
start_time = time.time()
whisper_model = whisper.load_model("small", device=device)
load_time = time.time() - start_time
print(f"‚úÖ Whisper –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {load_time:.2f}—Å")

# –ó–∞–≥—Ä—É–∂–∞–µ–º SpeechBrain –¥–ª—è speaker recognition
print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º SpeechBrain ECAPA...")
start_time = time.time()

# –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
target_dir = Path("pretrained_models/spkrec-ecapa-voxceleb")
target_dir.mkdir(parents=True, exist_ok=True)

required_files = ["hyperparams.yaml", "embedding_model.ckpt", "classifier.ckpt", "label_encoder.txt", "mean_var_norm_emb.ckpt"]
print(f"üìã –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏...")
for filename in required_files:
    target_file = target_dir / filename
    if not target_file.exists():
        try:
            cached_file = huggingface_hub.hf_hub_download(
                repo_id="speechbrain/spkrec-ecapa-voxceleb",
                filename=filename,
                cache_dir=str(Path.home() / ".cache" / "huggingface")
            )
            import shutil
            shutil.copy2(cached_file, target_file)
        except Exception as e:
            if "404" not in str(e):
                print(f"  ‚ö†Ô∏è  {filename}: {e}")

speaker_model = SpeakerRecognition.from_hparams(
    source="pretrained_models/spkrec-ecapa-voxceleb",
    savedir="pretrained_models/spkrec-ecapa-voxceleb",
    run_opts={"device": device}
)
load_time = time.time() - start_time
print(f"‚úÖ SpeechBrain –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {load_time:.2f}—Å\n")

print("=" * 80)
print(f"üåê WebSocket —Å–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –Ω–∞ ws://0.0.0.0:8766")
print(f"üìä –†–µ–∂–∏–º: {device.upper()}")
print(f"üé≠ Speaker Recognition: ENABLED")
print("=" * 80)
print()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
SAMPLE_RATE = 16000

# Hotwords –¥–ª—è boosting —Å –≤–µ—Å–∞–º–∏ (—á–µ–º –≤—ã—à–µ –≤–µ—Å, —Ç–µ–º —Å–∏–ª—å–Ω–µ–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
HOTWORDS = "Kiko:10.0, kiko:10.0, –∫–∏–∫–æ:8.0, –∫—ñ–∫–æ:8.0"

# Initial prompt –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (0% –Ω–∞–≥—Ä—É–∑–∫–∏)
INITIAL_PROMPT = "Kiko is a voice assistant. Common words: Kiko, hello, play, stop, volume, turn on, turn off."

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è post-correction (<1ms –Ω–∞–≥—Ä—É–∑–∫–∏)
CORRECTION_DICT = {
    "kiko": "Kiko",
    "kyko": "Kiko",
    "keeko": "Kiko",
    "kico": "Kiko",
    "kieko": "Kiko",
    "keyko": "Kiko",
    "tico": "Kiko",
    "tiko": "Kiko",
}

SPEAKERS_DB_FILE = "speakers_database.pkl"

# –ë–∞–∑–∞ —Å–ø–∏–∫–µ—Ä–æ–≤
speakers_database = {}
if os.path.exists(SPEAKERS_DB_FILE):
    try:
        with open(SPEAKERS_DB_FILE, 'rb') as f:
            speakers_database = pickle.load(f)
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(speakers_database)} —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤\n")
    except Exception as e:
        print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É: {e}\n")


def save_speakers_database():
    try:
        with open(SPEAKERS_DB_FILE, 'wb') as f:
            pickle.dump(speakers_database, f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑—ã: {e}")


def apply_post_correction(text):
    """–ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Å—Ç-–∫–æ—Ä—Ä–µ–∫—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ (<1ms)"""
    if not text:
        return text
    
    import re
    from difflib import get_close_matches
    
    words = text.split()
    corrected_words = []
    
    for word in words:
        # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        clean_word = re.sub(r'[^\w\s]', '', word).lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if clean_word in CORRECTION_DICT:
            # –ó–∞–º–µ–Ω—è–µ–º, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
            corrected = word.replace(clean_word, CORRECTION_DICT[clean_word])
            corrected = corrected.replace(clean_word.capitalize(), CORRECTION_DICT[clean_word])
            corrected_words.append(corrected)
        # Fuzzy match –¥–ª—è –æ–ø–µ—á–∞—Ç–æ–∫
        elif len(clean_word) > 2:
            matches = get_close_matches(clean_word, CORRECTION_DICT.keys(), n=1, cutoff=0.8)
            if matches:
                corrected = word.replace(clean_word, CORRECTION_DICT[matches[0]])
                corrected = corrected.replace(clean_word.capitalize(), CORRECTION_DICT[matches[0]])
                corrected_words.append(corrected)
            else:
                corrected_words.append(word)
        else:
            corrected_words.append(word)
    
    return ' '.join(corrected_words)


def simple_noise_gate(audio_data, threshold=0.01):
    """–ü—Ä–æ—Å—Ç–æ–π noise gate - –æ–±–Ω—É–ª—è–µ–º —Ç–∏—Ö–∏–µ —É—á–∞—Å—Ç–∫–∏ (–ø–æ—á—Ç–∏ 0ms)"""
    audio_abs = np.abs(audio_data)
    mask = audio_abs > threshold
    return audio_data * mask


def get_speaker_embedding(audio_np):
    try:
        audio_tensor = torch.from_numpy(audio_np).float()
        if device == "cuda":
            audio_tensor = audio_tensor.cuda()
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)
        embedding = speaker_model.encode_batch(audio_tensor)
        return embedding.squeeze()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
        return None


def identify_speaker(embedding, threshold=0.25):
    if embedding is None or len(speakers_database) == 0:
        return None, 0.0
    
    best_match = None
    best_similarity = -1.0
    
    for speaker_id, data in speakers_database.items():
        similarity = torch.nn.functional.cosine_similarity(
            embedding.unsqueeze(0),
            data["embedding"].unsqueeze(0)
        ).item()
        
        if similarity > best_similarity:
            best_similarity = similarity
            best_match = speaker_id
    
    if best_similarity > threshold:
        return best_match, best_similarity
    
    return None, best_similarity


def register_speaker(name, embedding):
    speaker_id = f"speaker_{len(speakers_database) + 1}"
    speakers_database[speaker_id] = {
        "name": name,
        "embedding": embedding,
        "samples_count": 1,
        "created_at": datetime.now().isoformat()
    }
    save_speakers_database()
    return speaker_id


def update_speaker_embedding(speaker_id, new_embedding, alpha=0.3):
    if speaker_id in speakers_database:
        old_embedding = speakers_database[speaker_id]["embedding"]
        updated_embedding = alpha * new_embedding + (1 - alpha) * old_embedding
        speakers_database[speaker_id]["embedding"] = updated_embedding
        speakers_database[speaker_id]["samples_count"] += 1
        save_speakers_database()


async def handle_client(websocket):
    client_id = f"{websocket.remote_address[0]}:{websocket.remote_address[1]}"
    print(f"üéôÔ∏è  –ü–æ–¥–∫–ª—é—á–∏–ª—Å—è: {client_id}")
    
    audio_buffer = []
    
    try:
        await websocket.send(json.dumps({
            "type": "connected",
            "message": "STT + Speaker Recognition server ready",
            "device": device,
            "sample_rate": SAMPLE_RATE,
            "speakers_count": len(speakers_database)
        }))
        
        async for message in websocket:
            data = json.loads(message)
            msg_type = data.get("type", "audio")
            
            if msg_type == "audio":
                audio_chunk = np.frombuffer(
                    base64.b64decode(data["audio"]),
                    dtype=np.int16
                ).astype(np.float32) / 32768.0
                audio_buffer.append(audio_chunk)
            
            elif msg_type == "finalize":
                if not audio_buffer:
                    await websocket.send(json.dumps({
                        "type": "transcription",
                        "text": "",
                        "is_final": True,
                        "timestamp": datetime.now().isoformat()
                    }))
                    continue
                
                audio = np.concatenate(audio_buffer)
                audio_buffer = []
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ—Å—Ç–æ–π noise gate (–ø–æ—á—Ç–∏ 0ms –Ω–∞–≥—Ä—É–∑–∫–∏)
                audio = simple_noise_gate(audio, threshold=0.01)
                
                duration = len(audio) / SAMPLE_RATE
                
                # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å hotwords –∏ initial_prompt
                start_time = time.time()
                result = whisper_model.transcribe(
                    audio,
                    language="en",
                    initial_prompt=INITIAL_PROMPT,  # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ (+0-2ms)
                    fp16=True
                )
                text = result["text"].strip()
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º post-correction (<1ms)
                text = apply_post_correction(text)
                
                transcribe_time = time.time() - start_time
                
                # Speaker recognition
                speaker_start = time.time()
                embedding = get_speaker_embedding(audio)
                speaker_id, similarity = identify_speaker(embedding)
                speaker_time = time.time() - speaker_start
                
                speaker_info = None
                if speaker_id:
                    speaker_info = {
                        "id": speaker_id,
                        "name": speakers_database[speaker_id]["name"],
                        "similarity": round(similarity, 3),
                        "is_known": True
                    }
                    update_speaker_embedding(speaker_id, embedding)
                else:
                    speaker_info = {
                        "id": "unknown",
                        "name": "Unknown Speaker",
                        "similarity": round(similarity, 3),
                        "is_known": False
                    }
                
                rtf = transcribe_time / duration if duration > 0 else 0
                
                print(f"üß† [{client_id}] {speaker_info['name']}: {text!r}")
                print(f"‚è±Ô∏è  {duration:.2f}s –∞—É–¥–∏–æ ‚Üí {transcribe_time*1000:.0f}ms STT + {speaker_time*1000:.0f}ms speaker (RTF: {rtf:.3f}x)")
                
                response = {
                    "type": "transcription",
                    "text": text,
                    "is_final": True,
                    "language": result.get("language", "ru"),
                    "timestamp": datetime.now().isoformat(),
                    "speaker": speaker_info,
                    "metrics": {
                        "audio_duration_s": round(duration, 3),
                        "transcription_time_s": round(transcribe_time, 3),
                        "transcription_time_ms": round(transcribe_time * 1000, 2),
                        "speaker_recognition_time_ms": round(speaker_time * 1000, 2),
                        "realtime_factor": round(rtf, 3),
                        "samples": len(audio)
                    }
                }
                
                await websocket.send(json.dumps(response))
            
            elif msg_type == "register_speaker":
                speaker_name = data.get("name", "Unknown")
                
                if not audio_buffer:
                    await websocket.send(json.dumps({
                        "type": "error",
                        "message": "No audio data for speaker registration"
                    }))
                    continue
                
                audio = np.concatenate(audio_buffer)
                audio_buffer = []
                
                embedding = get_speaker_embedding(audio)
                if embedding is not None:
                    speaker_id = register_speaker(speaker_name, embedding)
                    print(f"‚úÖ –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω: {speaker_name} (ID: {speaker_id})")
                    
                    await websocket.send(json.dumps({
                        "type": "speaker_registered",
                        "speaker_id": speaker_id,
                        "name": speaker_name
                    }))
                else:
                    await websocket.send(json.dumps({
                        "type": "error",
                        "message": "Failed to extract speaker embedding"
                    }))
            
            elif msg_type == "list_speakers":
                speakers_list = [
                    {
                        "id": sid,
                        "name": sdata["name"],
                        "samples_count": sdata["samples_count"],
                        "created_at": sdata["created_at"]
                    }
                    for sid, sdata in speakers_database.items()
                ]
                
                await websocket.send(json.dumps({
                    "type": "speakers_list",
                    "speakers": speakers_list,
                    "total": len(speakers_list)
                }))
    
    except websockets.exceptions.ConnectionClosed:
        print(f"üëã –û—Ç–∫–ª—é—á–∏–ª—Å—è: {client_id}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ [{client_id}]: {e}")
        import traceback
        traceback.print_exc()


async def main():
    host = "0.0.0.0"
    port = 8766
    
    print(f"üéß –û–∂–∏–¥–∞—é –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π...")
    
    async with websockets.serve(handle_client, host, port, max_size=10_000_000):
        await asyncio.Future()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nüëã –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
