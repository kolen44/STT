"""
WebSocket STT —Å–µ—Ä–≤–µ—Ä v2.0 - ChatGPT-style –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ä–µ–∂–∏–º
OpenAI Whisper Small –Ω–∞ GPU

–û—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—É–∑ (–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ 0.5-1.5 —Å–µ–∫ –≤–º–µ—Å—Ç–æ 6 —Å–µ–∫)
- Streaming partial results –≤–æ –≤—Ä–µ–º—è —Ä–µ—á–∏
- Proper sentence boundary detection
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö "Kiko"
- Real-time feedback –∫–∞–∫ –≤ ChatGPT
"""
import warnings
warnings.filterwarnings("ignore")

import asyncio
import websockets
import json
import whisper
import torch
import numpy as np
from datetime import datetime
import base64
import time
from collections import defaultdict
import re
from difflib import get_close_matches
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass, field
from enum import Enum
import threading

# ============ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ============
print("=" * 80)
print("üöÄ WEBSOCKET STT SUPER SERVER v2.0 (ChatGPT-style)")
print("=" * 80)

# –§–æ—Ä—Å–∏—Ä—É–µ–º GPU —Ä–µ–∂–∏–º
if not torch.cuda.is_available():
    print("‚ùå CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ CUDA Toolkit –∏ GPU –¥—Ä–∞–π–≤–µ—Ä—ã")
    print("   https://developer.nvidia.com/cuda-downloads")
    import sys
    sys.exit(1)

device = "cuda"
print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º Whisper Small
print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º Whisper Small ({device.upper()})...")
start_time = time.time()
whisper_model = whisper.load_model("small", device=device)
load_time = time.time() - start_time
print(f"‚úÖ Whisper –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {load_time:.2f}—Å\n")

print("=" * 80)
print(f"üåê WebSocket —Å–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –Ω–∞ ws://0.0.0.0:8765")
print(f"üìä –†–µ–∂–∏–º: {device.upper()} | ChatGPT-style –¥–∏–∞–ª–æ–≥")
print("=" * 80)
print()

# ===============================
# –ù–ê–°–¢–†–û–ô–ö–ò - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –î–õ–Ø –î–ò–ê–õ–û–ì–ê
# ===============================
SAMPLE_RATE = 16000
BYTES_PER_SAMPLE = 2  # int16


# === VAD –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—É–∑) ===
class VADConfig:
    # –ü–æ—Ä–æ–≥ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏
    ENERGY_THRESHOLD = 0.012  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è –ª—É—á—à–µ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–∏—à–∏–Ω—ã
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ (–∑–∞—â–∏—Ç–∞ –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π)
    MIN_AUDIO_ENERGY = 0.015  # –ï—Å–ª–∏ —ç–Ω–µ—Ä–≥–∏—è –Ω–∏–∂–µ - –Ω–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –≤–æ–æ–±—â–µ
    
    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—É–∑—ã - –£–í–ï–õ–ò–ß–ï–ù–´ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–Ω—Ü–∞ —Ñ—Ä–∞–∑
    MIN_PAUSE_MS = 800        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑ ("–¥–∞", "–Ω–µ—Ç")
    DEFAULT_PAUSE_MS = 1000   # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö —Ñ—Ä–∞–∑
    MAX_PAUSE_MS = 1800       # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    QUESTION_PAUSE_MS = 800   # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    MIN_SPEECH_MS = 300       # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —à—É–º–æ–≤
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞
    MAX_SEGMENT_MS = 30000    # 30 —Å–µ–∫—É–Ω–¥ –º–∞–∫—Å
    
    # –ß–∞—Å—Ç–æ—Ç–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ partial results
    PARTIAL_INTERVAL_MS = 400  # –ö–∞–∂–¥—ã–µ 400–º—Å - –æ—Ç–∑—ã–≤—á–∏–≤–æ
    
    # –†–∞–∑–º–µ—Ä VAD —Ñ—Ä–µ–π–º–∞
    FRAME_MS = 30             # 30–º—Å —Ñ—Ä–µ–π–º—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–∫–ª–∏–∫–∞
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–µ–π–º–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏
    SPEECH_START_FRAMES = 2   # 2 —Ñ—Ä–µ–π–º–∞ = 60–º—Å –¥–ª—è —Å—Ç–∞—Ä—Ç–∞


# === Hotwords –¥–ª—è boosting ===
HOTWORDS = ["Kiko", "kiko", "KIKO", "–∫–∏–∫–æ", "–∫—ñ–∫–æ", "–ö–∏–∫–æ"]

# Initial prompt –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
INITIAL_PROMPT = "Kiko is a voice assistant. The user is having a conversation with Kiko. Common phrases: Hey Kiko, Kiko help, Kiko search, play music, what time is it, tell me about."

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è post-correction
CORRECTION_DICT = {
    # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    "kiko": "Kiko", "kyko": "Kiko", "keeko": "Kiko", "kico": "Kiko",
    "kieko": "Kiko", "keyko": "Kiko", "tico": "Kiko", "tiko": "Kiko",
    "keco": "Kiko", "cico": "Kiko", "qico": "Kiko", "kika": "Kiko",
    "kikko": "Kiko", "keko": "Kiko", "chico": "Kiko",
    # –†—É—Å—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    "–∫–∏–∫–æ": "Kiko", "–∫—ñ–∫–æ": "Kiko", "–∫–∏–∫–∞": "Kiko", "–∫–µ–∫–æ": "Kiko", "—Ç–∏–∫–æ": "Kiko",
}

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ —Ñ—Ä–∞–∑—ã
QUESTION_PATTERNS = [
    r'\?$',
    r'^(what|who|where|when|why|how|can|could|would|should|is|are|do|does|did)\b',
    r'^(—á—Ç–æ|–∫—Ç–æ|–≥–¥–µ|–∫–æ–≥–¥–∞|–ø–æ—á–µ–º—É|–∫–∞–∫|–º–æ–∂–Ω–æ|–º–æ–≥—É)\b',
]

COMMAND_PATTERNS = [
    r'^(play|stop|pause|next|previous|volume|mute|unmute)\b',
    r'^(–≤–∫–ª—é—á–∏|–≤—ã–∫–ª—é—á–∏|–ø–æ—Å—Ç–∞–≤—å|—Å–ª–µ–¥—É—é—â|–ø—Ä–µ–¥—ã–¥—É—â|–≥—Ä–æ–º–∫–æ—Å—Ç—å)\b',
    r'^(search|find|show|open|close|start|turn)\b',
    r'^(–Ω–∞–π–¥–∏|–ø–æ–∫–∞–∂–∏|–æ—Ç–∫—Ä–æ–π|–∑–∞–∫—Ä–æ–π|–∑–∞–ø—É—Å—Ç–∏)\b',
]

SHORT_RESPONSE_PATTERNS = [
    r'^(yes|no|ok|okay|sure|yeah|yep|nope|maybe)$',
    r'^(–¥–∞|–Ω–µ—Ç|–æ–∫–µ–π|–ª–∞–¥–Ω–æ|—Ö–æ—Ä–æ—à–æ|–º–æ–∂–µ—Ç)$',
]


# ===============================
# –°–û–°–¢–û–Ø–ù–ò–ï –ö–õ–ò–ï–ù–¢–ê
# ===============================
class SpeechState(Enum):
    SILENCE = 0      # –¢–∏—à–∏–Ω–∞, –æ–∂–∏–¥–∞–Ω–∏–µ
    SPEECH = 1       # –ê–∫—Ç–∏–≤–Ω–∞—è —Ä–µ—á—å
    PAUSE = 2        # –ü–∞—É–∑–∞ –≤ —Ä–µ—á–∏ (–º–æ–∂–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å—Å—è)


@dataclass
class ClientSession:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π —Å–µ—Å—Å–∏–∏"""
    client_id: str
    
    # –ê—É–¥–∏–æ –±—É—Ñ–µ—Ä—ã
    audio_buffer: List[np.ndarray] = field(default_factory=list)
    speech_buffer: List[np.ndarray] = field(default_factory=list)
    
    # –°–æ—Å—Ç–æ—è–Ω–∏–µ VAD
    state: SpeechState = SpeechState.SILENCE
    speech_frames: int = 0
    silence_frames: int = 0
    
    # Timing
    speech_start_time: float = 0.0
    last_partial_time: float = 0.0
    pause_start_time: float = 0.0
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø–∞—É–∑
    last_transcript: str = ""
    conversation_context: List[str] = field(default_factory=list)
    
    # Speaker tracking
    speaker_sessions: Dict[str, int] = field(default_factory=dict)
    speaker_counter: int = 0
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    total_speech_ms: float = 0.0
    total_segments: int = 0


# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–µ—Å—Å–∏–π
sessions: Dict[str, ClientSession] = {}
sessions_lock = threading.Lock()


# ===============================
# –£–¢–ò–õ–ò–¢–´
# ===============================

def calculate_energy(audio: np.ndarray) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç RMS —ç–Ω–µ—Ä–≥–∏—é –∞—É–¥–∏–æ"""
    if len(audio) == 0:
        return 0.0
    return float(np.sqrt(np.mean(audio ** 2)))


def calculate_zero_crossings(audio: np.ndarray) -> int:
    """–ü–æ–¥—Å—á—ë—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –Ω—É–ª—è"""
    if len(audio) < 2:
        return 0
    return int(np.sum(np.abs(np.diff(np.sign(audio))) > 0))


def is_speech_frame(audio: np.ndarray) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ—Ä–µ–π–º —Ä–µ—á—å—é"""
    if len(audio) == 0:
        return False
    
    energy = calculate_energy(audio)
    if energy < VADConfig.ENERGY_THRESHOLD:
        return False
    
    # –®—É–º –∏–º–µ–µ—Ç –º–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –Ω—É–ª—è, —Ä–µ—á—å - –º–µ–Ω—å—à–µ
    zc_rate = calculate_zero_crossings(audio) / len(audio) if len(audio) > 0 else 0
    if zc_rate > 0.5:
        return False
    
    return True


def determine_pause_duration(text: str, speech_duration_ms: float) -> int:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–π –ø–∞—É–∑—ã.
    –ö–∞–∫ –≤ ChatGPT - –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.
    """
    text_lower = text.lower().strip()
    
    # 1. –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã - –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞
    for pattern in SHORT_RESPONSE_PATTERNS:
        if re.match(pattern, text_lower, re.IGNORECASE):
            return VADConfig.MIN_PAUSE_MS
    
    # 2. –ö–æ–º–∞–Ω–¥—ã - –∫–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞
    for pattern in COMMAND_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            return VADConfig.MIN_PAUSE_MS + 100
    
    # 3. –í–æ–ø—Ä–æ—Å—ã - —Å—Ä–µ–¥–Ω—è—è –ø–∞—É–∑–∞
    for pattern in QUESTION_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            return VADConfig.QUESTION_PAUSE_MS
    
    # 4. –ù–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–±–µ–∑ –∑–Ω–∞–∫–∞ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ)
    if text_lower and not re.search(r'[.!?,:;]$', text_lower):
        return VADConfig.MAX_PAUSE_MS
    
    # 5. –ü–æ –¥–ª–∏–Ω–µ —Ä–µ—á–∏
    if speech_duration_ms < 1000:
        return VADConfig.MIN_PAUSE_MS
    elif speech_duration_ms < 3000:
        return VADConfig.DEFAULT_PAUSE_MS
    else:
        return VADConfig.MAX_PAUSE_MS


def apply_post_correction(text: str) -> str:
    """–ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Å—Ç-–∫–æ—Ä—Ä–µ–∫—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö Kiko"""
    if not text:
        return text
    
    words = text.split()
    corrected_words = []
    
    for word in words:
        clean_word = re.sub(r'[^\w\s]', '', word).lower()
        punctuation_after = re.sub(r'^[\w\s]+', '', word)
        punctuation_before = re.sub(r'[\w\s]+$', '', word)
        
        corrected = None
        
        if clean_word in CORRECTION_DICT:
            corrected = CORRECTION_DICT[clean_word]
        elif len(clean_word) > 2:
            matches = get_close_matches(clean_word, CORRECTION_DICT.keys(), n=1, cutoff=0.75)
            if matches:
                corrected = CORRECTION_DICT[matches[0]]
        
        if corrected:
            final_word = punctuation_before + corrected + punctuation_after
            corrected_words.append(final_word)
        else:
            corrected_words.append(word)
    
    result = ' '.join(corrected_words)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã Kiko —Ä—è–¥–æ–º: "Kiko Kiko –≤–∫–ª—é—á–∏" -> "Kiko, –≤–∫–ª—é—á–∏"
    result = re.sub(r'\bKiko\s+Kiko\b', 'Kiko,', result, flags=re.IGNORECASE)
    
    return result


def get_speaker_hash(audio_data: np.ndarray) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ø–∏–∫–µ—Ä–∞"""
    if len(audio_data) == 0:
        return "unknown"
    
    mean_amplitude = np.mean(np.abs(audio_data))
    std_amplitude = np.std(audio_data)
    zero_crossings = calculate_zero_crossings(audio_data)
    
    fft = np.fft.rfft(audio_data)
    magnitude = np.abs(fft)
    
    spectral_centroid = 0
    if np.sum(magnitude) > 0:
        spectral_centroid = np.sum(magnitude * np.arange(len(magnitude))) / np.sum(magnitude)
    
    n = len(magnitude)
    low_freq = np.sum(magnitude[:n//4]) if n >= 4 else 0
    high_freq = np.sum(magnitude[3*n//4:]) if n >= 4 else 0
    
    return f"{mean_amplitude:.5f}_{std_amplitude:.5f}_{zero_crossings}_{spectral_centroid:.2f}_{low_freq:.2f}_{high_freq:.2f}"


# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π Whisper (—Ç–µ–∫—Å—Ç –∏–∑ –ø—Ä–æ–º–ø—Ç–∞ –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è)
HALLUCINATION_PATTERNS = [
    r'^kiko[\s,\.]*kiko[\s,\.]*kiko',  # –ü–æ–≤—Ç–æ—Ä—è—é—â–µ–µ—Å—è Kiko
    r'^(kiko[\s,\.]*){{3,}}',  # Kiko 3+ —Ä–∞–∑ –ø–æ–¥—Ä—è–¥
    r'^–∫–∏–∫–æ[\s,\.]*–∫–∏–∫–æ[\s,\.]*–∫–∏–∫–æ',  # –¢–æ –∂–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    r'voice assistant',  # –ò–∑ –ø—Ä–æ–º–ø—Ç–∞
    r'common phrases',  # –ò–∑ –ø—Ä–æ–º–ø—Ç–∞  
    r'having a conversation',  # –ò–∑ –ø—Ä–æ–º–ø—Ç–∞
    r'^\s*\.+\s*$',  # –¢–æ–ª—å–∫–æ —Ç–æ—á–∫–∏
    r'^\s*,+\s*$',  # –¢–æ–ª—å–∫–æ –∑–∞–ø—è—Ç—ã–µ
    r'thank you for watching',  # –¢–∏–ø–∏—á–Ω–∞—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è
    r'thanks for watching',
    r'subscribe',
    r'like and subscribe',
    r'please subscribe',
]

def is_noise_or_garbage(text: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —à—É–º–æ–º –∏–ª–∏ –º—É—Å–æ—Ä–æ–º"""
    if not text:
        return True
    
    t = text.strip()
    if len(t) < 2:
        return True
    if re.match(r'^\[[^\]]+\]$', t):  # [BLANK_AUDIO], [MUSIC], etc.
        return True
    if re.match(r'^[\s\.,!?\-\‚Äî\‚Äì\'\"‚Ä¶]+$', t):
        return True
    if re.match(r'^(.)\1{2,}$', t):
        return True
    
    return False


def is_hallucination(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–µ–π Whisper"""
    if not text:
        return False
    
    t = text.lower().strip()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    for pattern in HALLUCINATION_PATTERNS:
        if re.search(pattern, t, re.IGNORECASE):
            return True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞ ("kiko kiko kiko" –∏–ª–∏ "the the the")
    words = t.split()
    if len(words) >= 3:
        # –ï—Å–ª–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è 3+ —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥
        for i in range(len(words) - 2):
            if words[i] == words[i+1] == words[i+2]:
                return True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ Kiko (–±–æ–ª—å—à–µ 2 –≤ –∫–æ—Ä–æ—Ç–∫–æ–º —Ç–µ–∫—Å—Ç–µ)
    kiko_count = len(re.findall(r'\bkiko\b', t, re.IGNORECASE))
    word_count = len(words)
    if word_count > 0 and kiko_count > 2 and kiko_count / word_count > 0.5:
        return True
    
    return False


def has_sufficient_audio_energy(audio: np.ndarray) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –≤ –∞—É–¥–∏–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è —Ä–µ—á–∏"""
    if len(audio) == 0:
        return False
    
    energy = calculate_energy(audio)
    
    # –ï—Å–ª–∏ —Å—Ä–µ–¥–Ω—è—è —ç–Ω–µ—Ä–≥–∏—è —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è - —ç—Ç–æ —Ç–∏—à–∏–Ω–∞
    if energy < VADConfig.MIN_AUDIO_ENERGY:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –∫–∞–∫–∏–µ-—Ç–æ "–ø–∏–∫–∏" (—Ä–µ—á—å –∏–º–µ–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É)
    max_amplitude = np.max(np.abs(audio))
    if max_amplitude < 0.05:  # –ï—Å–ª–∏ –º–∞–∫—Å –∞–º–ø–ª–∏—Ç—É–¥–∞ < 5% - —ç—Ç–æ —Ç–∏—à–∏–Ω–∞/—à—É–º
        return False
    
    return True


# ===============================
# –ì–õ–ê–í–ù–ê–Ø –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò
# ===============================

async def transcribe_audio(audio: np.ndarray, session: ClientSession) -> Tuple[str, dict]:
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –∑–∞—â–∏—Ç–æ–π –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."""
    audio_duration = len(audio) / SAMPLE_RATE
    
    # –ó–ê–©–ò–¢–ê –û–¢ –ì–ê–õ–õ–Æ–¶–ò–ù–ê–¶–ò–ô: –ø—Ä–æ–≤–µ—Ä—è–µ–º —ç–Ω–µ—Ä–≥–∏—é –∞—É–¥–∏–æ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π
    if not has_sufficient_audio_energy(audio):
        print(f"‚ö†Ô∏è [{session.client_id}] Audio energy too low, skipping transcription")
        return "", {"transcription_time_ms": 0, "audio_duration_s": round(audio_duration, 3), 
                   "realtime_factor": 0, "samples": len(audio), "skipped": "low_energy"}
    
    # Noise gate
    audio = audio * (np.abs(audio) > 0.008)  # –£–≤–µ–ª–∏—á–µ–Ω –ø–æ—Ä–æ–≥
    
    start_time = time.perf_counter()
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ñ—Ä–∞–∑ (–Ω–æ –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º INITIAL_PROMPT —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç
    context_prompt = "Kiko assistant."
    if session.conversation_context:
        recent = session.conversation_context[-2:]  # –ú–µ–Ω—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_prompt = f"Kiko. {' '.join(recent)}"
    
    result = whisper_model.transcribe(
        audio,
        language="en",
        initial_prompt=context_prompt,
        fp16=True,
        condition_on_previous_text=False,  # –û–¢–ö–õ–Æ–ß–ï–ù–û –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
        no_speech_threshold=0.6,  # –£–≤–µ–ª–∏—á–µ–Ω –ø–æ—Ä–æ–≥ "–Ω–µ—Ç —Ä–µ—á–∏"
        logprob_threshold=-0.8,   # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    )
    
    text = result["text"].strip()
    text = apply_post_correction(text)
    
    end_time = time.perf_counter()
    transcription_time = (end_time - start_time) * 1000
    rtf = audio_duration / (transcription_time / 1000) if transcription_time > 0 else 0
    
    # –ü–†–û–í–ï–†–ö–ê –ù–ê –ì–ê–õ–õ–Æ–¶–ò–ù–ê–¶–ò–ò
    if is_hallucination(text):
        print(f"üö´ [{session.client_id}] Hallucination filtered: {text!r}")
        return "", {"transcription_time_ms": round(transcription_time, 2), 
                   "audio_duration_s": round(audio_duration, 3),
                   "realtime_factor": round(rtf, 2), "samples": len(audio), 
                   "filtered": "hallucination", "original_text": text}
    
    metrics = {
        "transcription_time_ms": round(transcription_time, 2),
        "audio_duration_s": round(audio_duration, 3),
        "realtime_factor": round(rtf, 2),
        "samples": len(audio),
    }
    
    return text, metrics


async def process_vad_frame(session: ClientSession, frame: np.ndarray, websocket) -> Optional[dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω VAD —Ñ—Ä–µ–π–º. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∏–ª–∏ None."""
    is_speech = is_speech_frame(frame)
    current_time = time.time()
    
    result = None
    
    if is_speech:
        session.speech_frames += 1
        session.silence_frames = 0
        
        if session.state == SpeechState.SILENCE:
            if session.speech_frames >= VADConfig.SPEECH_START_FRAMES:
                session.state = SpeechState.SPEECH
                session.speech_start_time = current_time
                session.speech_buffer = list(session.audio_buffer[-5:])  # Preroll
                print(f"üé§ [{session.client_id}] Speech started")
        
        elif session.state == SpeechState.PAUSE:
            session.state = SpeechState.SPEECH
            print(f"üé§ [{session.client_id}] Speech resumed")
        
        if session.state == SpeechState.SPEECH:
            session.speech_buffer.append(frame)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º partial —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if current_time - session.last_partial_time > VADConfig.PARTIAL_INTERVAL_MS / 1000:
                session.last_partial_time = current_time
                
                if len(session.speech_buffer) > 0:
                    audio = np.concatenate(session.speech_buffer)
                    if len(audio) > SAMPLE_RATE * 0.3:
                        text, _ = await transcribe_audio(audio, session)
                        if text and not is_noise_or_garbage(text):
                            session.last_transcript = text
                            result = {
                                "type": "partial",
                                "text": text,
                                "is_final": False,
                                "timestamp": datetime.now().isoformat(),
                            }
    else:
        session.silence_frames += 1
        session.speech_frames = max(0, session.speech_frames - 1)
        
        if session.state == SpeechState.SPEECH:
            session.speech_buffer.append(frame)
            
            if session.silence_frames >= 3:  # ~90–º—Å —Ç–∏—à–∏–Ω—ã -> –ø–µ—Ä–µ—Ö–æ–¥ –≤ –ø–∞—É–∑—É
                session.state = SpeechState.PAUSE
                session.pause_start_time = current_time
        
        elif session.state == SpeechState.PAUSE:
            session.speech_buffer.append(frame)
            
            speech_duration_ms = (current_time - session.speech_start_time) * 1000
            pause_duration_ms = session.silence_frames * VADConfig.FRAME_MS
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –ø–∞—É–∑—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞
            required_pause = VADConfig.DEFAULT_PAUSE_MS
            if session.last_transcript:
                required_pause = determine_pause_duration(session.last_transcript, speech_duration_ms)
            
            # –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –µ—Å–ª–∏ –ø–∞—É–∑–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è
            if pause_duration_ms >= required_pause:
                result = await finalize_segment(session)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
    max_samples = int(VADConfig.MAX_SEGMENT_MS * SAMPLE_RATE / 1000)
    if session.state in (SpeechState.SPEECH, SpeechState.PAUSE):
        if sum(len(b) for b in session.speech_buffer) > max_samples:
            result = await finalize_segment(session)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—Ä–µ–π–º –¥–ª—è preroll
    session.audio_buffer.append(frame)
    if len(session.audio_buffer) > 10:
        session.audio_buffer.pop(0)
    
    return result


async def finalize_segment(session: ClientSession) -> Optional[dict]:
    """–§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç —Ä–µ—á–∏"""
    if not session.speech_buffer:
        return None
    
    audio = np.concatenate(session.speech_buffer)
    duration_ms = len(audio) / SAMPLE_RATE * 1000
    
    # –°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è
    session.speech_buffer = []
    session.state = SpeechState.SILENCE
    session.speech_frames = 0
    session.silence_frames = 0
    
    if duration_ms < VADConfig.MIN_SPEECH_MS:
        print(f"‚è≠Ô∏è [{session.client_id}] Segment too short ({duration_ms:.0f}ms), skipping")
        return None
    
    # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê —ç–Ω–µ—Ä–≥–∏–∏ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π
    if not has_sufficient_audio_energy(audio):
        print(f"‚ö†Ô∏è [{session.client_id}] Audio energy too low ({duration_ms:.0f}ms), skipping")
        return None
    
    # –°–ø–∏–∫–µ—Ä
    speaker_hash = get_speaker_hash(audio)
    if speaker_hash not in session.speaker_sessions:
        session.speaker_counter += 1
        session.speaker_sessions[speaker_hash] = session.speaker_counter
    speaker_num = session.speaker_sessions[speaker_hash]
    
    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
    text, metrics = await transcribe_audio(audio, session)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–ª–∏ –º—É—Å–æ—Ä
    if not text or is_noise_or_garbage(text):
        print(f"üóëÔ∏è [{session.client_id}] Empty or garbage filtered: {text!r}")
        return None
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
    if is_hallucination(text):
        print(f"üö´ [{session.client_id}] Hallucination in final: {text!r}")
        return None
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    session.last_transcript = text
    session.conversation_context.append(text)
    if len(session.conversation_context) > 10:
        session.conversation_context.pop(0)
    
    session.total_speech_ms += duration_ms
    session.total_segments += 1
    
    print(f"üìù [{session.client_id}] Speaker #{speaker_num}: {text!r}")
    print(f"‚è±Ô∏è  Duration: {duration_ms:.0f}ms | Transcription: {metrics['transcription_time_ms']:.0f}ms | RTF: {metrics['realtime_factor']:.1f}x")
    
    return {
        "type": "transcription",
        "text": text,
        "is_final": True,
        "timestamp": datetime.now().isoformat(),
        "speaker_number": speaker_num,
        "metrics": metrics,
    }


# ===============================
# WS HANDLER
# ===============================

async def handle_client(websocket):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞"""
    client_id = str(id(websocket))
    
    # –°–æ–∑–¥–∞—ë–º —á–∏—Å—Ç—É—é —Å–µ—Å—Å–∏—é (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ)
    session = ClientSession(client_id=client_id)
    with sessions_lock:
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é —Å–µ—Å—Å–∏—é –µ—Å–ª–∏ –±—ã–ª–∞ (–ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏)
        if client_id in sessions:
            print(f"‚ôªÔ∏è [{client_id}] Cleaning up previous session")
            del sessions[client_id]
        sessions[client_id] = session
    
    print(f"üîå –ö–ª–∏–µ–Ω—Ç –ø–æ–¥–∫–ª—é—á–∏–ª—Å—è: {client_id}")
    
    frame_samples = int(SAMPLE_RATE * VADConfig.FRAME_MS / 1000)
    pcm_buffer = np.array([], dtype=np.float32)
    
    # –°—á—ë—Ç—á–∏–∫ —Ç–∏—à–∏–Ω—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ
    silence_streak = 0
    MAX_SILENCE_BEFORE_SKIP = 50  # ~1.5 —Å–µ–∫ —Ç–∏—à–∏–Ω—ã –ø–æ–¥—Ä—è–¥ = —Å–∫–∏–ø–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    
    try:
        await websocket.send(json.dumps({
            "type": "connected",
            "message": "ChatGPT-style STT server ready (v2.1 anti-hallucination)",
            "sample_rate": SAMPLE_RATE,
            "model": "whisper-small",
            "device": device,
            "features": [
                "adaptive_pause_detection",
                "streaming_partials",
                "speaker_identification",
                "hallucination_filter",
            ],
        }))
        
        async for message in websocket:
            try:
                data = json.loads(message)
                msg_type = data.get("type")
                
                if msg_type == "audio":
                    audio_b64 = data.get("audio") or ""
                    if not audio_b64:
                        continue
                    
                    audio_chunk = np.frombuffer(
                        base64.b64decode(audio_b64),
                        dtype=np.int16
                    ).astype(np.float32) / 32768.0
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–Ω–µ—Ä–≥–∏—é —á–∞–Ω–∫–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç–∏—à–∏–Ω—ã
                    chunk_energy = calculate_energy(audio_chunk)
                    if chunk_energy < VADConfig.ENERGY_THRESHOLD:
                        silence_streak += 1
                    else:
                        silence_streak = 0
                    
                    # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ —Ç–∏—à–∏–Ω–∞ - —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
                    if silence_streak > MAX_SILENCE_BEFORE_SKIP:
                        if session.state != SpeechState.SILENCE:
                            print(f"üîá [{client_id}] Long silence detected, resetting buffers")
                            session.speech_buffer = []
                            session.state = SpeechState.SILENCE
                            session.speech_frames = 0
                            session.silence_frames = 0
                        pcm_buffer = np.array([], dtype=np.float32)
                        continue
                    
                    pcm_buffer = np.concatenate([pcm_buffer, audio_chunk])
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ—Ä–µ–π–º—ã
                    while len(pcm_buffer) >= frame_samples:
                        frame = pcm_buffer[:frame_samples]
                        pcm_buffer = pcm_buffer[frame_samples:]
                        
                        result = await process_vad_frame(session, frame, websocket)
                        if result:
                            await websocket.send(json.dumps(result))
                
                elif msg_type == "finalize":
                    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
                    if session.state != SpeechState.SILENCE:
                        result = await finalize_segment(session)
                        if result:
                            await websocket.send(json.dumps(result))
                        else:
                            await websocket.send(json.dumps({
                                "type": "transcription",
                                "text": "",
                                "is_final": True,
                                "timestamp": datetime.now().isoformat(),
                            }))
                    else:
                        if len(pcm_buffer) > frame_samples:
                            session.speech_buffer = [pcm_buffer]
                            result = await finalize_segment(session)
                            pcm_buffer = np.array([], dtype=np.float32)
                            if result:
                                await websocket.send(json.dumps(result))
                            else:
                                await websocket.send(json.dumps({
                                    "type": "transcription",
                                    "text": "",
                                    "is_final": True,
                                    "timestamp": datetime.now().isoformat(),
                                }))
                        else:
                            await websocket.send(json.dumps({
                                "type": "transcription",
                                "text": "",
                                "is_final": True,
                                "timestamp": datetime.now().isoformat(),
                            }))
                
                elif msg_type == "ping":
                    await websocket.send(json.dumps({
                        "type": "pong",
                        "timestamp": datetime.now().isoformat(),
                    }))
                
                elif msg_type == "reset":
                    session.conversation_context = []
                    session.last_transcript = ""
                    await websocket.send(json.dumps({
                        "type": "reset_ack",
                        "timestamp": datetime.now().isoformat(),
                    }))
                
                elif msg_type == "config":
                    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    if "pause_ms" in data:
                        VADConfig.DEFAULT_PAUSE_MS = int(data["pause_ms"])
                    if "energy_threshold" in data:
                        VADConfig.ENERGY_THRESHOLD = float(data["energy_threshold"])
                    await websocket.send(json.dumps({
                        "type": "config_ack",
                        "config": {
                            "pause_ms": VADConfig.DEFAULT_PAUSE_MS,
                            "energy_threshold": VADConfig.ENERGY_THRESHOLD,
                        },
                    }))
            
            except json.JSONDecodeError:
                await websocket.send(json.dumps({
                    "type": "error",
                    "message": "Invalid JSON",
                }))
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                import traceback
                traceback.print_exc()
                await websocket.send(json.dumps({
                    "type": "error",
                    "message": str(e),
                }))
    
    except websockets.exceptions.ConnectionClosed:
        print(f"üîå –ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è: {client_id}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ handle_client: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print(f"üëã –°–µ—Å—Å–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {client_id}")
        print(f"   üìä –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {session.total_segments}")
        print(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è —Ä–µ—á–∏: {session.total_speech_ms/1000:.1f}—Å")
        print(f"   üé≠ –°–ø–∏–∫–µ—Ä–æ–≤: {session.speaker_counter}")
        
        with sessions_lock:
            if client_id in sessions:
                del sessions[client_id]


async def main():
    """–ó–∞–ø—É—Å–∫ WebSocket —Å–µ—Ä–≤–µ—Ä–∞"""
    server = await websockets.serve(
        handle_client,
        "0.0.0.0",
        8765,
        ping_interval=20,
        ping_timeout=20,
        max_size=10 * 1024 * 1024  # 10MB max message size
    )
    
    print("üéß –û–∂–∏–¥–∞—é –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π...")
    await server.wait_closed()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nüëã –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
