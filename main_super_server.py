"""
WebSocket STT —Å–µ—Ä–≤–µ—Ä v2.6 - Wake word "Optimus"
OpenAI Whisper Medium –Ω–∞ GPU + Picovoice Porcupine Wake Word

–£–ª—É—á—à–µ–Ω–∏—è v2.6:
- –ó–∞–º–µ–Ω–∞ wake word "Kiko" –Ω–∞ "Optimus" –≤–æ –≤—Å—ë–º –∫–æ–¥–µ
- –û–±–Ω–æ–≤–ª–µ–Ω—ã CORRECTION_DICT, PHONETIC_VARIANTS, HOTWORDS
- –û–±–Ω–æ–≤–ª–µ–Ω—ã —Ñ—É–Ω–∫—Ü–∏–∏ fuzzy_match, check_first_word, clean_duplicate
"""
import warnings
warnings.filterwarnings("ignore")

import asyncio
import websockets
import websockets.exceptions
import json
import whisper
import torch
import numpy as np
from datetime import datetime
import base64
import time
from collections import defaultdict
import re
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass, field
from enum import Enum
import threading
import gc
from concurrent.futures import ThreadPoolExecutor
import signal
import sys as _sys

# ============ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ============
print("=" * 80)
print("üöÄ WEBSOCKET STT SUPER SERVER v2.0 (ChatGPT-style)")
print("=" * 80)

# –§–æ—Ä—Å–∏—Ä—É–µ–º GPU —Ä–µ–∂–∏–º
if not torch.cuda.is_available():
    print("‚ùå CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ CUDA Toolkit –∏ GPU –¥—Ä–∞–π–≤–µ—Ä—ã")
    print("   https://developer.nvidia.com/cuda-downloads")
    import sys
    sys.exit(1)

device = "cuda"
print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º Whisper Medium - –±–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å
print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º Whisper Medium ({device.upper()})...")
start_time = time.time()
whisper_model = whisper.load_model("medium", device=device)
load_time = time.time() - start_time
print(f"‚úÖ Whisper –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {load_time:.2f}—Å\n")

print("=" * 80)
print(f"üåê WebSocket —Å–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –Ω–∞ ws://0.0.0.0:8765")
print(f"üìä –†–µ–∂–∏–º: {device.upper()} | ChatGPT-style –¥–∏–∞–ª–æ–≥")
print("=" * 80)
print()

# ThreadPoolExecutor –¥–ª—è –±–ª–æ–∫–∏—Ä—É—é—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (Whisper)
# –ò—Å–ø–æ–ª—å–∑—É–µ–º 1 –≤–æ—Ä–∫–µ—Ä - –º–æ–¥–µ–ª—å –ù–ï thread-safe!
whisper_executor = ThreadPoolExecutor(max_workers=1, thread_name_prefix="whisper_")

# Lock –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ Whisper –º–æ–¥–µ–ª–∏ (CUDA –Ω–µ thread-safe)
whisper_lock = threading.Lock()

# –¢–∞–π–º–∞—É—Ç –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ (—Å–µ–∫—É–Ω–¥—ã)
TRANSCRIBE_TIMEOUT = 30.0

# –ò–Ω—Ç–µ—Ä–≤–∞–ª –æ—á–∏—Å—Ç–∫–∏ GPU –ø–∞–º—è—Ç–∏ (—Å–µ–∫—É–Ω–¥—ã)
GPU_CLEANUP_INTERVAL = 60.0
last_gpu_cleanup = time.time()

# ===============================
# –ù–ê–°–¢–†–û–ô–ö–ò - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ö–ê–ö –£ OPENAI AUDIO
# ===============================
SAMPLE_RATE = 16000
BYTES_PER_SAMPLE = 2  # int16

# === WHISPER ADVANCED SETTINGS - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –ö–ê–ß–ï–°–¢–í–û v2.2 ===
class WhisperConfig:
    # Beam search - –£–í–ï–õ–ò–ß–ï–ù–û –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    BEAM_SIZE = 7      # 7 beams - –≤—ã—à–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑
    BEST_OF = 7        # –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∏–∑ 7 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    
    # Temperature - –ù–ò–ó–ö–ê–Ø –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏, –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π fallback
    TEMPERATURE = (0.0, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0)  # –ë–æ–ª—å—à–µ —à–∞–≥–æ–≤ –¥–ª—è fallback
    
    # Compression ratio - –ú–Ø–ì–ß–ï –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑
    COMPRESSION_RATIO_THRESHOLD = 2.8
    
    # Log probability - –ï–©–Å –ú–Ø–ì–ß–ï –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
    LOGPROB_THRESHOLD = -1.5  # –ú—è–≥—á–µ - –ø—Ä–∏–Ω–∏–º–∞–µ–º –º–µ–Ω–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
    
    # No speech threshold - –ú–Ø–ì–ß–ï –¥–ª—è —Ç–∏—Ö–æ–π —Ä–µ—á–∏
    NO_SPEECH_THRESHOLD = 0.55  # –ù–∏–∂–µ - –º–µ–Ω—å—à–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ —Ç–∏—Ö–æ–π —Ä–µ—á–∏
    
    # Condition on previous - –æ—Ç–∫–ª—é—á–µ–Ω–æ –¥–ª—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    CONDITION_ON_PREVIOUS = False
    
    # Word timestamps - –í–ö–õ–Æ–ß–ï–ù–û –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ word-level confidence
    WORD_TIMESTAMPS = True  # –£–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å + –¥–∞—ë—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø–æ —Å–ª–æ–≤–∞–º
    
    # Punctuations - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä
    PREPEND_PUNCTUATIONS = "\"'¬ø([{-¬´"
    APPEND_PUNCTUATIONS = "\"'.;:?!,ÿå„ÄÅ„ÄÇ„Äç„Äè„Äë„Äó„ÄãÔºâ¬ª\n"


# === VAD –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –ß–£–í–°–¢–í–ò–¢–ï–õ–¨–ù–û–°–¢–¨ v2.2 ===
class VADConfig:
    # –ü–æ—Ä–æ–≥ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏ - ULTRA –ú–Ø–ì–ö–ò–ô
    ENERGY_THRESHOLD = 0.002  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ —à—ë–ø–æ—Ç–∞
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ - ULTRA –ú–Ø–ì–ö–ò–ô
    MIN_AUDIO_ENERGY = 0.003  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π –¥–ª—è —Ç–∏—Ö–æ–≥–æ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
    
    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—É–∑—ã - –ë–´–°–¢–†–ï–ï –¥–ª—è –æ—Ç–∑—ã–≤—á–∏–≤–æ—Å—Ç–∏
    MIN_PAUSE_MS = 650        # 650–º—Å –º–∏–Ω–∏–º—É–º - –±—ã—Å—Ç—Ä–µ–µ —Ä–µ–∞–∫—Ü–∏—è
    DEFAULT_PAUSE_MS = 1000   # 1000–º—Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç - –±—ã—Å—Ç—Ä–µ–µ
    MAX_PAUSE_MS = 1500       # 1500–º—Å –º–∞–∫—Å –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    QUESTION_PAUSE_MS = 750   # 750–º—Å –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ - –±—ã—Å—Ç—Ä–µ–µ
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏ - –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª—è Whisper
    MIN_SPEECH_MS = 500       # 500–º—Å - –º–∏–Ω–∏–º—É–º –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞
    MAX_SEGMENT_MS = 30000    # 30 —Å–µ–∫—É–Ω–¥
    
    # –ü–æ—Ä–æ–≥ –¥–ª—è –º—è–≥–∫–æ–π —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    SOFT_SEGMENT_MS = 20000   # 20 —Å–µ–∫
    
    # –ß–∞—Å—Ç–æ—Ç–∞ partial - –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞
    PARTIAL_INTERVAL_MS = 300  # 300–º—Å - —Ä–µ–∂–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    
    # –†–∞–∑–º–µ—Ä VAD —Ñ—Ä–µ–π–º–∞
    FRAME_MS = 20             # 20–º—Å —Ñ—Ä–µ–π–º—ã
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–µ–π–º–æ–≤ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏ - –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    SPEECH_START_FRAMES = 2   # 2 —Ñ—Ä–µ–π–º–∞ = 40–º—Å - —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ
    
    # –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø
    DEDUP_WINDOW_MS = 2500    # 2.5 —Å–µ–∫—É–Ω–¥—ã


# === Hotwords –¥–ª—è boosting ===
HOTWORDS = ["Optimus", "optimus", "OPTIMUS", "–æ–ø—Ç–∏–º—É—Å", "–û–ø—Ç–∏–º—É—Å"]

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ preroll —Ñ—Ä–µ–π–º–æ–≤ - –ú–ê–ö–°–ò–ú–£–ú –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ —Å–ª–æ–≤
PREROLL_FRAMES = 30  # 30 —Ñ—Ä–µ–π–º–æ–≤ = 600–º—Å preroll –¥–ª—è –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –∑–∞—Ö–≤–∞—Ç–∞ –Ω–∞—á–∞–ª–∞

# –°–ª–æ–≤–∞—Ä—å –¥–ª—è post-correction - –†–ê–°–®–ò–†–ï–ù–ù–´–ô v2.2
# –í–∫–ª—é—á–∞–µ—Ç –≤—Å–µ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã Optimus –∏ —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏ Whisper
CORRECTION_DICT = {
    # –ü—Ä—è–º—ã–µ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã Optimus
    "optimus": "Optimus", "optimas": "Optimus", "optimis": "Optimus", "optimes": "Optimus",
    "optimus'": "Optimus", "optimus's": "Optimus", "optimous": "Optimus", "optimis": "Optimus",
    "optimus,": "Optimus,", "optimus.": "Optimus.", "optimus?": "Optimus?", "optimus!": "Optimus!",
    "optimuss": "Optimus", "optimuz": "Optimus", "optimuse": "Optimus", "optimust": "Optimus",
    "optumus": "Optimus", "optames": "Optimus", "optemos": "Optimus", "optimis": "Optimus",
    # –î–≤—É—Ö—Å–ª–æ–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    "hey optimus": "Optimus", "ok optimus": "Optimus", "okay optimus": "Optimus",
    "hi optimus": "Optimus", "oh optimus": "Optimus", "yo optimus": "Optimus",
    # –†—É—Å—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    "–æ–ø—Ç–∏–º—É—Å": "Optimus", "–æ–ø—Ç—ñ–º—É—Å": "Optimus", "–æ–ø—Ç–∏–º–∞—Å": "Optimus", "–æ–ø—Ç–∏–º–æ—Å": "Optimus",
    "–æ–ø—Ç–∏–º—É—Å.": "Optimus", "–æ–ø—Ç–∏–º—É—Å,": "Optimus", "–æ–ø—Ç—ñ–º—É—Å.": "Optimus",
}

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ —Ñ—Ä–∞–∑—ã
QUESTION_PATTERNS = [
    r'\?$',
    r'^(what|who|where|when|why|how|can|could|would|should|is|are|do|does|did)\b',
    r'^(—á—Ç–æ|–∫—Ç–æ|–≥–¥–µ|–∫–æ–≥–¥–∞|–ø–æ—á–µ–º—É|–∫–∞–∫|–º–æ–∂–Ω–æ|–º–æ–≥—É)\b',
]

COMMAND_PATTERNS = [
    r'^(play|stop|pause|next|previous|volume|mute|unmute)\b',
    r'^(–≤–∫–ª—é—á–∏|–≤—ã–∫–ª—é—á–∏|–ø–æ—Å—Ç–∞–≤—å|—Å–ª–µ–¥—É—é—â|–ø—Ä–µ–¥—ã–¥—É—â|–≥—Ä–æ–º–∫–æ—Å—Ç—å)\b',
    r'^(search|find|show|open|close|start|turn)\b',
    r'^(–Ω–∞–π–¥–∏|–ø–æ–∫–∞–∂–∏|–æ—Ç–∫—Ä–æ–π|–∑–∞–∫—Ä–æ–π|–∑–∞–ø—É—Å—Ç–∏)\b',
]

SHORT_RESPONSE_PATTERNS = [
    r'^(yes|no|ok|okay|sure|yeah|yep|nope|maybe)$',
    r'^(–¥–∞|–Ω–µ—Ç|–æ–∫–µ–π|–ª–∞–¥–Ω–æ|—Ö–æ—Ä–æ—à–æ|–º–æ–∂–µ—Ç)$',
]


# ===============================
# –°–û–°–¢–û–Ø–ù–ò–ï –ö–õ–ò–ï–ù–¢–ê
# ===============================
class SpeechState(Enum):
    SILENCE = 0      # –¢–∏—à–∏–Ω–∞, –æ–∂–∏–¥–∞–Ω–∏–µ
    SPEECH = 1       # –ê–∫—Ç–∏–≤–Ω–∞—è —Ä–µ—á—å
    PAUSE = 2        # –ü–∞—É–∑–∞ –≤ —Ä–µ—á–∏ (–º–æ–∂–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å—Å—è)


@dataclass
class ClientSession:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π —Å–µ—Å—Å–∏–∏"""
    client_id: str
    
    # –í—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏
    created_at: float = field(default_factory=time.time)
    last_activity: float = field(default_factory=time.time)
    
    # –ê—É–¥–∏–æ –±—É—Ñ–µ—Ä—ã
    audio_buffer: List[np.ndarray] = field(default_factory=list)
    speech_buffer: List[np.ndarray] = field(default_factory=list)
    
    # –°–æ—Å—Ç–æ—è–Ω–∏–µ VAD
    state: SpeechState = SpeechState.SILENCE
    speech_frames: int = 0
    silence_frames: int = 0
    
    # Timing
    speech_start_time: float = 0.0
    last_partial_time: float = 0.0
    pause_start_time: float = 0.0
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø–∞—É–∑
    last_transcript: str = ""
    conversation_context: List[str] = field(default_factory=list)
    
    # Speaker tracking
    speaker_sessions: Dict[str, int] = field(default_factory=dict)
    speaker_counter: int = 0
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    total_speech_ms: float = 0.0
    total_segments: int = 0
    
    # –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—É—é –æ—Ç–ø—Ä–∞–≤–∫—É –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    last_sent_text: str = ""
    last_sent_time: float = 0.0


# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–µ—Å—Å–∏–π
sessions: Dict[str, ClientSession] = {}
sessions_lock = asyncio.Lock()  # asyncio Lock –≤–º–µ—Å—Ç–æ threading Lock –¥–ª—è async –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ—Å—Å–∏–π (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏)
MAX_CONCURRENT_SESSIONS = 50

# –¢–∞–π–º–∞—É—Ç –Ω–µ–∞–∫—Ç–∏–≤–Ω–æ–π —Å–µ—Å—Å–∏–∏ (—Å–µ–∫—É–Ω–¥—ã) - —Å–µ—Å—Å–∏–∏ –±–µ–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —É–¥–∞–ª—è—é—Ç—Å—è
SESSION_IDLE_TIMEOUT = 120.0  # 2 –º–∏–Ω—É—Ç—ã


async def cleanup_stale_sessions():
    """–£–¥–∞–ª—è–µ—Ç –∑–∞–≤–∏—Å—à–∏–µ —Å–µ—Å—Å–∏–∏ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–≥–æ–µ –≤—Ä–µ–º—è"""
    current_time = time.time()
    stale_ids = []
    
    async with sessions_lock:
        for client_id, session in sessions.items():
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º last_activity –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            idle_time = current_time - session.last_activity
            
            if idle_time > SESSION_IDLE_TIMEOUT:
                stale_ids.append(client_id)
        
        # –£–¥–∞–ª—è–µ–º –∑–∞–≤–∏—Å—à–∏–µ —Å–µ—Å—Å–∏–∏
        for client_id in stale_ids:
            print(f"üóëÔ∏è [{client_id}] Removing stale session (idle > {SESSION_IDLE_TIMEOUT}s)")
            del sessions[client_id]
    
    if stale_ids:
        print(f"üßπ Cleaned up {len(stale_ids)} stale sessions")


async def cleanup_gpu_memory(force: bool = False):
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–µ–∫"""
    global last_gpu_cleanup
    current_time = time.time()
    
    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏–ª–∏ –ø–æ —Ç–∞–π–º–µ—Ä—É
    if force or current_time - last_gpu_cleanup > GPU_CLEANUP_INTERVAL:
        try:
            # –°–Ω–∞—á–∞–ª–∞ –æ—á–∏—â–∞–µ–º –∑–∞–≤–∏—Å—à–∏–µ —Å–µ—Å—Å–∏–∏
            await cleanup_stale_sessions()
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
            last_gpu_cleanup = current_time
            async with sessions_lock:
                active_count = len(sessions)
            print(f"üßπ GPU memory cleanup performed")
            print(f"üìä –ê–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π: {active_count}")
        except Exception as e:
            print(f"‚ö†Ô∏è GPU cleanup error: {e}")


# ===============================
# –£–¢–ò–õ–ò–¢–´
# ===============================

def calculate_energy(audio: np.ndarray) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç RMS —ç–Ω–µ—Ä–≥–∏—é –∞—É–¥–∏–æ"""
    if len(audio) == 0:
        return 0.0
    return float(np.sqrt(np.mean(audio ** 2)))


def calculate_zero_crossings(audio: np.ndarray) -> int:
    """–ü–æ–¥—Å—á—ë—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –Ω—É–ª—è"""
    if len(audio) < 2:
        return 0
    return int(np.sum(np.abs(np.diff(np.sign(audio))) > 0))


def is_speech_frame(audio: np.ndarray) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ—Ä–µ–π–º —Ä–µ—á—å—é"""
    if len(audio) == 0:
        return False
    
    energy = calculate_energy(audio)
    if energy < VADConfig.ENERGY_THRESHOLD:
        return False
    
    # –®—É–º –∏–º–µ–µ—Ç –º–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –Ω—É–ª—è, —Ä–µ—á—å - –º–µ–Ω—å—à–µ
    zc_rate = calculate_zero_crossings(audio) / len(audio) if len(audio) > 0 else 0
    if zc_rate > 0.5:
        return False
    
    return True


def determine_pause_duration(text: str, speech_duration_ms: float) -> int:
    """
    –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–û–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—É–∑—ã - –∫–∞–∫ —É ChatGPT Voice.
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–∫–ª–∏–∫–∞.
    """
    text_lower = text.lower().strip()
    words = text_lower.split()
    word_count = len(words)
    
    # 1. –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã (1-2 —Å–ª–æ–≤–∞) - –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞
    if word_count <= 2:
        return VADConfig.MIN_PAUSE_MS
    
    # 2. –Ø–≤–Ω–æ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Ç–æ—á–∫–∞, !, ?)
    if re.search(r'[.!?]$', text_lower):
        return VADConfig.MIN_PAUSE_MS + 50  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –¥–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    
    # 3. –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã - –±—ã—Å—Ç—Ä–æ
    for pattern in SHORT_RESPONSE_PATTERNS:
        if re.match(pattern, text_lower, re.IGNORECASE):
            return VADConfig.MIN_PAUSE_MS
    
    # 4. –ö–æ–º–∞–Ω–¥—ã - –±—ã—Å—Ç—Ä–æ
    for pattern in COMMAND_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            return VADConfig.MIN_PAUSE_MS + 50
    
    # 5. –í–æ–ø—Ä–æ—Å—ã - –Ω—É–∂–Ω–∞ –ø–∞—É–∑–∞ –ø–æ–±–æ–ª—å—à–µ —á—Ç–æ–±—ã —á–µ–ª–æ–≤–µ–∫ –¥–æ–≥–æ–≤–æ—Ä–∏–ª
    for pattern in QUESTION_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            return VADConfig.DEFAULT_PAUSE_MS  # 1000ms –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤
    
    # 6. –ù–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è - –∂–¥—ë–º –¥–æ–ª—å—à–µ
    incomplete_endings = ['–∏', '–∞', '–Ω–æ', '–∏–ª–∏', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', 
                         'and', 'or', 'but', 'that', 'which', 'who', 'where',
                         'the', 'a', 'an', 'to', 'for', 'with', 'in', 'on']
    
    if words and words[-1] in incomplete_endings:
        return VADConfig.MAX_PAUSE_MS
    
    if text_lower.endswith(','):
        return VADConfig.MAX_PAUSE_MS
    
    # 8. –ü–æ –¥–ª–∏–Ω–µ —Ä–µ—á–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤
    if word_count <= 4:
        return VADConfig.MIN_PAUSE_MS + 100
    elif word_count <= 8:
        return VADConfig.DEFAULT_PAUSE_MS
    else:
        # –î–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ - –≥–æ—Ç–æ–≤–æ
        if re.search(r'[.!?;]$', text_lower):
            return VADConfig.DEFAULT_PAUSE_MS
        return VADConfig.MAX_PAUSE_MS


# –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã Optimus - –¢–û–õ–¨–ö–û —è–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã, –±–µ–∑ –æ–±—ã—á–Ω—ã—Ö —Å–ª–æ–≤
OPTIMUS_PHONETIC_VARIANTS = [
    # –û—Å–Ω–æ–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã - –∑–≤—É—á–∞—Ç –∫–∞–∫ "optimus"
    "optimus", "optimas", "optimis", "optimes", "optimous", "optimuz",
    "optumus", "optemos", "optimuss", "optimuse", "optimust",
    # –†—É—Å—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    "–æ–ø—Ç–∏–º—É—Å", "–æ–ø—Ç—ñ–º—É—Å", "–æ–ø—Ç–∏–º–∞—Å", "–æ–ø—Ç–∏–º–æ—Å", "–æ–ø—Ç—ñ–º–∞—Å",
]


def fuzzy_match_optimus(word: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂–µ –ª–∏ —Å–ª–æ–≤–æ –Ω–∞ 'Optimus' - –°–¢–†–û–ì–ê–Ø –≤–µ—Ä—Å–∏—è"""
    clean = re.sub(r'[^\w]', '', word).lower()
    
    # –ü—É—Å—Ç–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ/–¥–ª–∏–Ω–Ω–æ–µ
    if not clean or len(clean) < 6 or len(clean) > 10:
        return False
    
    # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä–µ
    if clean in CORRECTION_DICT:
        return True
    
    # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã (—Å—Ç—Ä–æ–≥–∏–π —Å–ø–∏—Å–æ–∫)
    if clean in OPTIMUS_PHONETIC_VARIANTS:
        return True
    
    # –°—Ç—Ä–æ–≥–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω: opt + im/em/am + us/is/os
    if re.match(r'^opt[iea]m[uio]s+[est]?$', clean):
        return True
    
    # –†—É—Å—Å–∫–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω: –æ–ø—Ç + –∏–º/—ñ–º + —É—Å/–∞—Å
    if re.match(r'^–æ–ø—Ç[–∏—ñ–µ]–º[—É–∞–æ]—Å$', clean):
        return True
    
    return False


def apply_post_correction(text: str) -> str:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Å—Ç-–∫–æ—Ä—Ä–µ–∫—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ - –°–¢–†–û–ì–ê–Ø –≤–µ—Ä—Å–∏—è.
    –ó–∞–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã Optimus, –Ω–µ —Ç—Ä–æ–≥–∞–µ–º –æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞.
    """
    if not text:
        return text
    
    words = text.split()
    corrected_words = []
    
    for word in words:
        clean_word = re.sub(r'[^\w\s]', '', word).lower()
        punctuation_after = re.sub(r'^[\w\s]+', '', word)
        punctuation_before = re.sub(r'[\w\s]+$', '', word)
        
        corrected = None
        
        # –¢–û–õ–¨–ö–û –ø—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä–µ - –Ω–∏–∫–∞–∫–æ–≥–æ fuzzy matching!
        if clean_word in CORRECTION_DICT:
            corrected = CORRECTION_DICT[clean_word]
        # –°—Ç—Ä–æ–≥–∏–π fuzzy match —Ç–æ–ª—å–∫–æ –¥–ª—è —è–≤–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ Optimus
        elif fuzzy_match_optimus(clean_word):
            corrected = "Optimus"
        
        if corrected:
            final_word = punctuation_before + corrected + punctuation_after
            corrected_words.append(final_word)
        else:
            corrected_words.append(word)
    
    result = ' '.join(corrected_words)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã Optimus —Ä—è–¥–æ–º: "Optimus Optimus –≤–∫–ª—é—á–∏" -> "Optimus, –≤–∫–ª—é—á–∏"
    result = re.sub(r'\bOptimus\s+Optimus\b', 'Optimus,', result, flags=re.IGNORECASE)
    
    return result


def check_first_word_is_optimus(text: str) -> str:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –Ω–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç—å —Å Optimus –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –µ—Å–ª–∏ –Ω—É–∂–Ω–æ.
    –°–¢–†–û–ì–ê–Ø –≤–µ—Ä—Å–∏—è - —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã Optimus, –Ω–µ —Ç—Ä–æ–≥–∞–µ–º –æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞.
    """
    if not text or len(text) < 2:
        return text
    
    words = text.split()
    if not words:
        return text
    
    first_word = words[0].lower().strip('.,!?')
    
    # –†–ê–°–®–ò–†–ï–ù–ù–´–ô —Å–ø–∏—Å–æ–∫ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ Optimus
    optimus_like_starts = [
        # –ü—Ä—è–º—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–≤—É—á–∞—â–∏–µ –∫–∞–∫ "optimus"
        "optimus", "optimas", "optimis", "optimes", "optimous", "optimuz",
        "optumus", "optemos", "optimuss", "optimuse", "optimust",
        # –†—É—Å—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        "–æ–ø—Ç–∏–º—É—Å", "–æ–ø—Ç—ñ–º—É—Å", "–æ–ø—Ç–∏–º–∞—Å", "–æ–ø—Ç–∏–º–æ—Å", "–æ–ø—Ç–∏–º—É—Å,", "–æ–ø—Ç–∏–º—É—Å.",
    ]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ
    if first_word in optimus_like_starts:
        words[0] = "Optimus"
        return ' '.join(words)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¢–û–õ–¨–ö–û —è–≤–Ω—ã–µ –¥–≤—É—Ö—Å–ª–æ–≤–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å optimus
    if len(words) >= 2:
        two_words = f"{words[0]} {words[1]}".lower()
        optimus_like_two_words = [
            "hey optimus", "ok optimus", "okay optimus",
            "hi optimus", "oh optimus", "yo optimus",
            "—ç–π –æ–ø—Ç–∏–º—É—Å", "—Ö–µ–π –æ–ø—Ç–∏–º—É—Å", "–æ –æ–ø—Ç–∏–º—É—Å", "–ø—Ä–∏–≤–µ—Ç –æ–ø—Ç–∏–º—É—Å",
        ]
        if two_words in optimus_like_two_words:
            # –ó–∞–º–µ–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–∞ —Å–ª–æ–≤–∞ –Ω–∞ Optimus
            return "Optimus " + ' '.join(words[2:]) if len(words) > 2 else "Optimus"
    
    return text


def get_speaker_hash(audio_data: np.ndarray) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ø–∏–∫–µ—Ä–∞"""
    if len(audio_data) == 0:
        return "unknown"
    
    mean_amplitude = np.mean(np.abs(audio_data))
    std_amplitude = np.std(audio_data)
    zero_crossings = calculate_zero_crossings(audio_data)
    
    fft = np.fft.rfft(audio_data)
    magnitude = np.abs(fft)
    
    spectral_centroid = 0
    if np.sum(magnitude) > 0:
        spectral_centroid = np.sum(magnitude * np.arange(len(magnitude))) / np.sum(magnitude)
    
    n = len(magnitude)
    low_freq = np.sum(magnitude[:n//4]) if n >= 4 else 0
    high_freq = np.sum(magnitude[3*n//4:]) if n >= 4 else 0
    
    return f"{mean_amplitude:.5f}_{std_amplitude:.5f}_{zero_crossings}_{spectral_centroid:.2f}_{low_freq:.2f}_{high_freq:.2f}"


# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π Whisper (—Ç–µ–∫—Å—Ç –∏–∑ –ø—Ä–æ–º–ø—Ç–∞ –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è)
HALLUCINATION_PATTERNS = [
    r'^optimus[\s,\.]*optimus[\s,\.]*optimus',  # –ü–æ–≤—Ç–æ—Ä—è—é—â–µ–µ—Å—è Optimus 3+ —Ä–∞–∑
    r'^(optimus[\s,\.]*){4,}',  # Optimus 4+ —Ä–∞–∑ –ø–æ–¥—Ä—è–¥
    r'^–æ–ø—Ç–∏–º—É—Å[\s,\.]*–æ–ø—Ç–∏–º—É—Å[\s,\.]*–æ–ø—Ç–∏–º—É—Å',  # –¢–æ –∂–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    r'voice assistant',  # –ì–õ–ê–í–ù–´–ô –∏—Å—Ç–æ—á–Ω–∏–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π!
    r'optimus assistant',   # –ß–∞—Å—Ç–∞—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è
    r'optimus is a',        # –ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
    r'assistant optimus',   # –ï—â—ë –≤–∞—Ä–∏–∞–Ω—Ç
    r'common phrases',   # –ò–∑ –ø—Ä–æ–º–ø—Ç–∞  
    r'having a conversation',  # –ò–∑ –ø—Ä–æ–º–ø—Ç–∞
    r'^\s*\.+\s*$',     # –¢–æ–ª—å–∫–æ —Ç–æ—á–∫–∏
    r'^\s*,+\s*$',      # –¢–æ–ª—å–∫–æ –∑–∞–ø—è—Ç—ã–µ
    r'thank you for watching',  # –¢–∏–ø–∏—á–Ω–∞—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è YouTube
    r'thanks for watching',
    r'subscribe',
    r'like and subscribe',
    r'please subscribe',
    # –£–ë–†–ê–ù–û: r'^optimus\.?$' - —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π wake word!
    r'\bthe\s+optimus\b',  # "the Optimus" - –Ω–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ
]

def is_noise_or_garbage(text: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —à—É–º–æ–º –∏–ª–∏ –º—É—Å–æ—Ä–æ–º"""
    if not text:
        return True
    
    t = text.strip()
    if len(t) < 2:
        return True
    if re.match(r'^\[[^\]]+\]$', t):  # [BLANK_AUDIO], [MUSIC], etc.
        return True
    if re.match(r'^[\s\.,!?\-\‚Äî\‚Äì\'\"‚Ä¶]+$', t):
        return True
    if re.match(r'^(.)\1{2,}$', t):
        return True
    
    # –§–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–Ω—ã—Ö –∑–≤—É–∫–æ–≤: Grrrr, hmmm, aaah, etc.
    t_lower = t.lower().rstrip('!')
    # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –±—É–∫–≤—ã: grrrr, hmmm, aaaa
    if re.match(r'^([a-z])\1{2,}$', t_lower):
        return True
    # –ö–æ—Ä–æ—Ç–∫–∏–µ –º–µ–∂–¥–æ–º–µ—Ç–∏—è
    if t_lower in ['hmm', 'hm', 'uh', 'um', 'ah', 'oh', 'eh', 'mm', 'mhm', 'ugh', 'grr', 'grrr', 'grrrr', 'aah', 'ooh']:
        return True
    
    return False


def is_hallucination(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–µ–π Whisper"""
    if not text:
        return False
    
    t = text.lower().strip()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    for pattern in HALLUCINATION_PATTERNS:
        if re.search(pattern, t, re.IGNORECASE):
            return True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞ ("optimus optimus optimus" –∏–ª–∏ "the the the")
    words = t.split()
    if len(words) >= 3:
        # –ï—Å–ª–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è 3+ —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥
        for i in range(len(words) - 2):
            if words[i] == words[i+1] == words[i+2]:
                return True
    
    # –£–ë–†–ê–ù–û: –æ–¥–∏–Ω–æ—á–Ω–æ–µ "Optimus" - —ç—Ç–æ –≤–∞–ª–∏–¥–Ω—ã–π wake word!
    # –¢–µ–ø–µ—Ä—å –ù–ï —Ñ–∏–ª—å—Ç—Ä—É–µ–º –æ–¥–∏–Ω–æ—á–Ω–æ–µ Optimus - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    # non_optimus_words = [w for w in words if w.lower() != 'optimus']
    # if len(words) > 0 and len(non_optimus_words) == 0:
    #     return True
    
    return False


def clean_duplicate_optimus(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è 'optimus', –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π.
    –¢–∞–∫–∂–µ —É–±–∏—Ä–∞–µ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é —Ä—è–¥–æ–º —Å —É–¥–∞–ª—ë–Ω–Ω—ã–º–∏ optimus.
    """
    if not text:
        return text
    
    # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ optimus –≤ —Ç–µ–∫—Å—Ç–µ (–≤–∫–ª—é—á–∞—è —Å –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π —Ä—è–¥–æ–º)
    optimus_matches = list(re.finditer(r'\boptimus\b', text, re.IGNORECASE))
    if len(optimus_matches) <= 1:
        return text
    
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ optimus –∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ, –≤–º–µ—Å—Ç–µ —Å –æ–∫—Ä—É–∂–∞—é—â–µ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π
    result = text
    # –ò–¥—ë–º —Å –∫–æ–Ω—Ü–∞ —á—Ç–æ–±—ã –∏–Ω–¥–µ–∫—Å—ã –Ω–µ —Å–±–∏–≤–∞–ª–∏—Å—å
    for match in reversed(optimus_matches[1:]):
        start, end = match.start(), match.end()
        
        # –†–∞—Å—à–∏—Ä—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω —É–¥–∞–ª–µ–Ω–∏—è –Ω–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥
        while start > 0 and result[start-1] in ' ,.:;!?':
            start -= 1
        while end < len(result) and result[end] in ' ,.:;!?':
            end += 1
            
        result = result[:start] + ' ' + result[end:]
    
    # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ª–∏—à–Ω—é—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    result = re.sub(r'\s+', ' ', result)
    result = re.sub(r'\s*,\s*,+', ',', result)  # ,, -> ,
    result = re.sub(r'\s*\.\s*\.+', '.', result)  # .. -> .
    result = re.sub(r',\s*\.', '.', result)  # ,. -> .
    result = re.sub(r'\.\s*,', '.', result)  # ., -> .
    
    return result.strip()


def has_sufficient_audio_energy(audio: np.ndarray) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –≤ –∞—É–¥–∏–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è —Ä–µ—á–∏"""
    if len(audio) == 0:
        return False
    
    energy = calculate_energy(audio)
    
    # –ï—Å–ª–∏ —Å—Ä–µ–¥–Ω—è—è —ç–Ω–µ—Ä–≥–∏—è —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è - —ç—Ç–æ —Ç–∏—à–∏–Ω–∞
    if energy < VADConfig.MIN_AUDIO_ENERGY:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –∫–∞–∫–∏–µ-—Ç–æ "–ø–∏–∫–∏" (—Ä–µ—á—å –∏–º–µ–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É)
    max_amplitude = np.max(np.abs(audio))
    if max_amplitude < 0.05:  # –ï—Å–ª–∏ –º–∞–∫—Å –∞–º–ø–ª–∏—Ç—É–¥–∞ < 5% - —ç—Ç–æ —Ç–∏—à–∏–Ω–∞/—à—É–º
        return False
    
    return True


# ===============================
# –ì–õ–ê–í–ù–ê–Ø –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò
# ===============================

async def transcribe_audio(audio: np.ndarray, session: ClientSession, is_partial: bool = False) -> Tuple[str, dict]:
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –∑–∞—â–∏—Ç–æ–π –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç OpenAI-style –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.
    is_partial=True –ø–æ–¥–∞–≤–ª—è–µ—Ç verbose –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ.
    """
    audio_duration = len(audio) / SAMPLE_RATE
    
    # –ó–ê–©–ò–¢–ê –û–¢ –ì–ê–õ–õ–Æ–¶–ò–ù–ê–¶–ò–ô: –ø—Ä–æ–≤–µ—Ä—è–µ–º —ç–Ω–µ—Ä–≥–∏—é –∞—É–¥–∏–æ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π
    if not has_sufficient_audio_energy(audio):
        if not is_partial:  # –ù–µ —Å–ø–∞–º–∏–º –¥–ª—è partial
            print(f"‚ö†Ô∏è [{session.client_id}] Audio energy too low, skipping transcription")
        return "", {"transcription_time_ms": 0, "audio_duration_s": round(audio_duration, 3), 
                   "realtime_factor": 0, "samples": len(audio), "skipped": "low_energy"}
    
    # === –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –ê–£–î–ò–û v2.3 ===
    
    # –í–ê–ñ–ù–û: Whisper —Ç—Ä–µ–±—É–µ—Ç float32, —É–±–µ–∂–¥–∞–µ–º—Å—è –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ç–∏–ø–µ
    audio = audio.astype(np.float32)
    
    # 1. –£–±–∏—Ä–∞–µ–º DC offset (–ø–æ—Å—Ç–æ—è–Ω–Ω—É—é —Å–æ—Å—Ç–∞–≤–ª—è—é—â—É—é)
    audio = audio - np.mean(audio, dtype=np.float32)
    
    # 2. –ú—è–≥–∫–∏–π high-pass —Ñ–∏–ª—å—Ç—Ä –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ –≥—É–ª–∞ (< 80 Hz)
    # –ü—Ä–æ—Å—Ç–æ–π single-pole filter: y[n] = x[n] - x[n-1] + 0.97 * y[n-1]
    alpha = np.float32(0.97)
    filtered = np.zeros_like(audio, dtype=np.float32)
    for i in range(1, len(audio)):
        filtered[i] = audio[i] - audio[i-1] + alpha * filtered[i-1]
    audio = filtered
    
    # 3. –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏ (peak + RMS hybrid)
    max_val = np.max(np.abs(audio))
    rms = np.sqrt(np.mean(audio**2))
    
    if max_val > 0.01:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –ø–∏–∫—É, –Ω–æ —É—á–∏—Ç—ã–≤–∞–µ–º RMS –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –¥–∏–Ω–∞–º–∏–∫–∏
        target_rms = np.float32(0.15)  # –¶–µ–ª–µ–≤–æ–π RMS —É—Ä–æ–≤–µ–Ω—å
        if rms > 0.001:
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —É—Å–∏–ª–µ–Ω–∏–µ —á—Ç–æ–±—ã –Ω–µ –ø–æ–¥–Ω—è—Ç—å —à—É–º
            gain = np.float32(min(target_rms / rms, 0.95 / max_val, 3.0))
            audio = audio * gain
        else:
            audio = audio / max_val * np.float32(0.95)
    
    # 4. –ú—è–≥–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∏–∫–æ–≤ (soft clipping) –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫–ª–∏–ø–ø–∏–Ω–≥–∞
    audio = np.tanh(audio * np.float32(1.2)) / np.float32(np.tanh(1.2))
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ - –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û float32 –¥–ª—è Whisper!
    audio = audio.astype(np.float32)
    
    # –ü–†–û–ú–ü–¢ –û–¢–ö–õ–Æ–ß–Å–ù - –≤—ã–∑—ã–≤–∞–ª –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –∏ —É—Ö—É–¥—à–∞–ª —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
    # context_prompt = None
    
    start_time = time.perf_counter()
    
    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ executor - –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ï –ö–ê–ß–ï–°–¢–í–û
    # –í–ê–ñ–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º lock –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ GPU –º–æ–¥–µ–ª–∏
    def _transcribe_sync():
        # –ó–∞—â–∏—Ç–∞ –æ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –∞—É–¥–∏–æ (< 0.6 —Å–µ–∫) - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ
        if len(audio) < SAMPLE_RATE * 0.6:
            return {"text": "", "segments": []}
        
        # Lock –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–æ–¥–µ–ª–∏
        # –≠—Ç–æ —Ä–µ—à–∞–µ—Ç –æ—à–∏–±–∫–∏ "Key and Value must have the same sequence length"
        with whisper_lock:
            try:
                return whisper_model.transcribe(
                    audio,
                    language="en",  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —è–∑—ã–∫ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    task="transcribe",
                    # initial_prompt –û–¢–ö–õ–Æ–ß–Å–ù
                    fp16=True,
                    
                    # Beam search –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
                    beam_size=WhisperConfig.BEAM_SIZE,
                    best_of=WhisperConfig.BEST_OF,
                    
                    # Temperature - –Ω–∏–∑–∫–∞—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    temperature=WhisperConfig.TEMPERATURE,
                    
                    # –§–∏–ª—å—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞
                    compression_ratio_threshold=WhisperConfig.COMPRESSION_RATIO_THRESHOLD,
                    logprob_threshold=WhisperConfig.LOGPROB_THRESHOLD,
                    no_speech_threshold=WhisperConfig.NO_SPEECH_THRESHOLD,
                    
                    # –ù–µ–∑–∞–≤–∏—Å–∏–º—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
                    condition_on_previous_text=WhisperConfig.CONDITION_ON_PREVIOUS,
                    
                    # Word timestamps –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                    word_timestamps=WhisperConfig.WORD_TIMESTAMPS,
                    
                    # –ü—É–Ω–∫—Ç—É–∞—Ü–∏—è
                    prepend_punctuations=WhisperConfig.PREPEND_PUNCTUATIONS,
                    append_punctuations=WhisperConfig.APPEND_PUNCTUATIONS,
                )
            except RuntimeError as e:
                # –õ–æ–≤–∏–º CUDA/PyTorch –æ—à–∏–±–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                error_msg = str(e)
                if "sequence length" in error_msg or "size" in error_msg or "shape" in error_msg:
                    print(f"‚ö†Ô∏è [{session.client_id}] CUDA tensor error (recovering): {error_msg[:80]}")
                    # –û—á–∏—â–∞–µ–º GPU –∫—ç—à –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    torch.cuda.empty_cache()
                    return {"text": "", "segments": []}
                raise  # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –¥—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏
    
    try:
        # –í—ã–ø–æ–ª–Ω—è–µ–º –±–ª–æ–∫–∏—Ä—É—é—â—É—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
        loop = asyncio.get_event_loop()
        result = await asyncio.wait_for(
            loop.run_in_executor(whisper_executor, _transcribe_sync),
            timeout=TRANSCRIBE_TIMEOUT
        )
    except asyncio.TimeoutError:
        print(f"‚ö†Ô∏è [{session.client_id}] Transcription timeout after {TRANSCRIBE_TIMEOUT}s")
        return "", {"transcription_time_ms": TRANSCRIBE_TIMEOUT * 1000, 
                   "audio_duration_s": round(audio_duration, 3),
                   "error": "timeout"}
    except Exception as e:
        print(f"‚ùå [{session.client_id}] Transcription error: {e}")
        return "", {"transcription_time_ms": 0, 
                   "audio_duration_s": round(audio_duration, 3),
                   "error": str(e)}
    
    # –ó–∞—â–∏—Ç–∞ –æ—Ç None —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–º–æ–∂–µ—Ç —Å–ª—É—á–∏—Ç—å—Å—è –ø—Ä–∏ CUDA –æ—à–∏–±–∫–∞—Ö)
    if result is None:
        print(f"‚ö†Ô∏è [{session.client_id}] Transcription returned None")
        return "", {"transcription_time_ms": 0, 
                   "audio_duration_s": round(audio_duration, 3),
                   "error": "null_result"}
    
    text = result.get("text", "").strip() if isinstance(result, dict) else ""
    text = apply_post_correction(text)
    
    # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –µ—Å–ª–∏ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ Optimus - –∏—Å–ø—Ä–∞–≤–ª—è–µ–º
    original_first_word = text
    text = check_first_word_is_optimus(text)
    if text != original_first_word and not is_partial:
        print(f"üîß [{session.client_id}] Fixed first word to Optimus: {original_first_word!r} -> {text!r}")
    
    # –û—á–∏—â–∞–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è "optimus" (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π)
    original_text = text
    text = clean_duplicate_optimus(text)
    if text != original_text and not is_partial:
        print(f"üîß [{session.client_id}] Cleaned duplicate optimus: {original_text!r} -> {text!r}")
    
    end_time = time.perf_counter()
    transcription_time = (end_time - start_time) * 1000
    rtf = audio_duration / (transcription_time / 1000) if transcription_time > 0 else 0
    
    # –ü–†–û–í–ï–†–ö–ê –ù–ê –ì–ê–õ–õ–Æ–¶–ò–ù–ê–¶–ò–ò
    if is_hallucination(text):
        print(f"üö´ [{session.client_id}] Hallucination filtered: {text!r}")
        return "", {"transcription_time_ms": round(transcription_time, 2), 
                   "audio_duration_s": round(audio_duration, 3),
                   "realtime_factor": round(rtf, 2), "samples": len(audio), 
                   "filtered": "hallucination", "original_text": text}
    
    metrics = {
        "transcription_time_ms": round(transcription_time, 2),
        "audio_duration_s": round(audio_duration, 3),
        "realtime_factor": round(rtf, 2),
        "samples": len(audio),
    }
    
    return text, metrics


async def process_vad_frame(session: ClientSession, frame: np.ndarray, websocket) -> Optional[dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω VAD —Ñ—Ä–µ–π–º. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∏–ª–∏ None."""
    is_speech = is_speech_frame(frame)
    current_time = time.time()
    
    result = None
    
    if is_speech:
        session.speech_frames += 1
        session.silence_frames = 0
        
        if session.state == SpeechState.SILENCE:
            if session.speech_frames >= VADConfig.SPEECH_START_FRAMES:
                session.state = SpeechState.SPEECH
                session.speech_start_time = current_time
                # –£–í–ï–õ–ò–ß–ï–ù–ù–´–ô Preroll –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –Ω–∞—á–∞–ª–∞ —Ñ—Ä–∞–∑—ã —Å Optimus
                session.speech_buffer = list(session.audio_buffer[-PREROLL_FRAMES:])
                print(f"üé§ [{session.client_id}] Speech started (preroll: {len(session.speech_buffer)} frames)")
        
        elif session.state == SpeechState.PAUSE:
            session.state = SpeechState.SPEECH
            print(f"üé§ [{session.client_id}] Speech resumed")
        
        if session.state == SpeechState.SPEECH:
            session.speech_buffer.append(frame)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º partial —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if current_time - session.last_partial_time > VADConfig.PARTIAL_INTERVAL_MS / 1000:
                session.last_partial_time = current_time
                
                if len(session.speech_buffer) > 0:
                    audio = np.concatenate(session.speech_buffer)
                    # 400–º—Å –º–∏–Ω–∏–º—É–º –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ partial
                    if len(audio) > SAMPLE_RATE * 0.4:
                        text, _ = await transcribe_audio(audio, session, is_partial=True)
                        if text and not is_noise_or_garbage(text):
                            session.last_transcript = text
                            result = {
                                "type": "partial",
                                "text": text,
                                "is_final": False,
                                "timestamp": datetime.now().isoformat(),
                            }
    else:
        session.silence_frames += 1
        session.speech_frames = max(0, session.speech_frames - 1)
        
        if session.state == SpeechState.SPEECH:
            session.speech_buffer.append(frame)
            
            if session.silence_frames >= 2:  # ~40–º—Å —Ç–∏—à–∏–Ω—ã -> –ø–µ—Ä–µ—Ö–æ–¥ –≤ –ø–∞—É–∑—É (–±—ã—Å—Ç—Ä–µ–µ!)
                session.state = SpeechState.PAUSE
                session.pause_start_time = current_time
        
        elif session.state == SpeechState.PAUSE:
            session.speech_buffer.append(frame)
            
            speech_duration_ms = (current_time - session.speech_start_time) * 1000
            pause_duration_ms = session.silence_frames * VADConfig.FRAME_MS
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –ø–∞—É–∑—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞
            required_pause = VADConfig.DEFAULT_PAUSE_MS
            if session.last_transcript:
                required_pause = determine_pause_duration(session.last_transcript, speech_duration_ms)
            
            # –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –µ—Å–ª–∏ –ø–∞—É–∑–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è
            if pause_duration_ms >= required_pause:
                result = await finalize_segment(session)
            
            # CONTINUOUS MODE: –µ—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç –¥–ª–∏–Ω–Ω—ã–π –∏ –µ—Å—Ç—å –∫–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ - —Ä–∞–∑–±–∏–≤–∞–µ–º
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —á–∞—Å—Ç—è–º–∏ –¥–ª–∏–Ω–Ω—ã–µ –º–æ–Ω–æ–ª–æ–≥–∏ –±–µ–∑ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è
            elif speech_duration_ms > VADConfig.SOFT_SEGMENT_MS and pause_duration_ms >= 400:
                print(f"üì§ [{session.client_id}] Soft split at {speech_duration_ms:.0f}ms (continuous mode)")
                result = await finalize_segment(session, continue_listening=True)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ MAX - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ MAX
    max_samples = int(VADConfig.MAX_SEGMENT_MS * SAMPLE_RATE / 1000)
    if session.state in (SpeechState.SPEECH, SpeechState.PAUSE):
        if sum(len(b) for b in session.speech_buffer) > max_samples:
            print(f"üì§ [{session.client_id}] Hard split at {VADConfig.MAX_SEGMENT_MS}ms (continuous mode)")
            result = await finalize_segment(session, continue_listening=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—Ä–µ–π–º –¥–ª—è preroll - –£–í–ï–õ–ò–ß–ï–ù–ù–´–ô –±—É—Ñ–µ—Ä
    session.audio_buffer.append(frame)
    if len(session.audio_buffer) > PREROLL_FRAMES + 5:  # +5 –∑–∞–ø–∞—Å
        session.audio_buffer.pop(0)
    
    return result


async def finalize_segment(session: ClientSession, continue_listening: bool = False) -> Optional[dict]:
    """–§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç —Ä–µ—á–∏
    
    Args:
        session: –ö–ª–∏–µ–Ω—Ç—Å–∫–∞—è —Å–µ—Å—Å–∏—è
        continue_listening: –ï—Å–ª–∏ True - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª—É—à–∞—Ç—å –ø–æ—Å–ª–µ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ (continuous mode)
    """
    if not session.speech_buffer:
        return None
    
    audio = np.concatenate(session.speech_buffer)
    duration_ms = len(audio) / SAMPLE_RATE * 1000
    
    # –°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è
    session.speech_buffer = []
    session.speech_frames = 0
    session.silence_frames = 0
    
    # CONTINUOUS MODE: –æ—Å—Ç–∞—ë–º—Å—è –≤ —Ä–µ–∂–∏–º–µ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if continue_listening:
        session.state = SpeechState.SPEECH
        session.speech_start_time = time.time()
        print(f"üîÑ [{session.client_id}] Continuing to listen after segment...")
    else:
        session.state = SpeechState.SILENCE
    
    if duration_ms < VADConfig.MIN_SPEECH_MS:
        print(f"‚è≠Ô∏è [{session.client_id}] Segment too short ({duration_ms:.0f}ms), skipping")
        return None
    
    # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê —ç–Ω–µ—Ä–≥–∏–∏ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π
    if not has_sufficient_audio_energy(audio):
        print(f"‚ö†Ô∏è [{session.client_id}] Audio energy too low ({duration_ms:.0f}ms), skipping")
        return None
    
    # –°–ø–∏–∫–µ—Ä
    speaker_hash = get_speaker_hash(audio)
    if speaker_hash not in session.speaker_sessions:
        session.speaker_counter += 1
        session.speaker_sessions[speaker_hash] = session.speaker_counter
    speaker_num = session.speaker_sessions[speaker_hash]
    
    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
    text, metrics = await transcribe_audio(audio, session)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–ª–∏ –º—É—Å–æ—Ä
    if not text or is_noise_or_garbage(text):
        print(f"üóëÔ∏è [{session.client_id}] Empty or garbage filtered: {text!r}")
        return None
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
    if is_hallucination(text):
        print(f"üö´ [{session.client_id}] Hallucination in final: {text!r}")
        return None
    
    # –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –ª–∏ –º—ã —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –Ω–µ–¥–∞–≤–Ω–æ
    current_time = time.time()
    text_normalized = text.lower().strip()
    last_normalized = session.last_sent_text.lower().strip() if session.last_sent_text else ""
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–π —Ç–µ–∫—Å—Ç
    if last_normalized and text_normalized:
        time_since_last = (current_time - session.last_sent_time) * 1000
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∏–¥–µ–Ω—Ç–∏—á–µ–Ω –∏ –ø—Ä–æ—à–ª–æ –º–µ–Ω—å—à–µ DEDUP_WINDOW_MS
        if text_normalized == last_normalized and time_since_last < VADConfig.DEDUP_WINDOW_MS:
            print(f"üîÅ [{session.client_id}] Duplicate skipped: {text!r} (sent {time_since_last:.0f}ms ago)")
            return None
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂ (–æ–¥–∏–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –¥—Ä—É–≥–æ–π) –∏ –ø—Ä–æ—à–ª–æ –º–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
        if time_since_last < VADConfig.DEDUP_WINDOW_MS:
            if text_normalized in last_normalized or last_normalized in text_normalized:
                # –ï—Å–ª–∏ –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—á–µ –∏–ª–∏ —Ä–∞–≤–µ–Ω - –¥—É–±–ª–∏–∫–∞—Ç
                if len(text_normalized) <= len(last_normalized):
                    print(f"üîÅ [{session.client_id}] Partial duplicate skipped: {text!r}")
                    return None
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    session.last_sent_text = text
    session.last_sent_time = current_time
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    session.last_transcript = text
    session.conversation_context.append(text)
    if len(session.conversation_context) > 10:
        session.conversation_context.pop(0)
    
    session.total_speech_ms += duration_ms
    session.total_segments += 1
    
    print(f"üìù [{session.client_id}] Speaker #{speaker_num}: {text!r}")
    print(f"‚è±Ô∏è  Duration: {duration_ms:.0f}ms | Transcription: {metrics['transcription_time_ms']:.0f}ms | RTF: {metrics['realtime_factor']:.1f}x")
    
    return {
        "type": "transcription",
        "text": text,
        "is_final": True,
        "timestamp": datetime.now().isoformat(),
        "speaker_number": speaker_num,
        "metrics": metrics,
    }


# ===============================
# WS HANDLER
# ===============================

async def handle_client(websocket):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞"""
    client_id = str(id(websocket))
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞ —Å–µ—Å—Å–∏–π –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ GPU
    async with sessions_lock:
        current_count = len(sessions)
        if current_count >= MAX_CONCURRENT_SESSIONS:
            print(f"‚ö†Ô∏è [{client_id}] Rejected: too many sessions ({current_count}/{MAX_CONCURRENT_SESSIONS})")
            await websocket.send(json.dumps({
                "type": "error",
                "message": f"Server overloaded. Max {MAX_CONCURRENT_SESSIONS} concurrent sessions.",
                "code": "max_sessions_reached"
            }))
            await websocket.close()
            return
    
    # –°–æ–∑–¥–∞—ë–º —á–∏—Å—Ç—É—é —Å–µ—Å—Å–∏—é (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ)
    session = ClientSession(client_id=client_id)
    async with sessions_lock:
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é —Å–µ—Å—Å–∏—é –µ—Å–ª–∏ –±—ã–ª–∞ (–ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏)
        if client_id in sessions:
            print(f"‚ôªÔ∏è [{client_id}] Cleaning up previous session")
            del sessions[client_id]
        sessions[client_id] = session
    
    print(f"üîå –ö–ª–∏–µ–Ω—Ç –ø–æ–¥–∫–ª—é—á–∏–ª—Å—è: {client_id}")
    
    frame_samples = int(SAMPLE_RATE * VADConfig.FRAME_MS / 1000)
    pcm_buffer = np.array([], dtype=np.float32)
    
    # –°—á—ë—Ç—á–∏–∫ —Ç–∏—à–∏–Ω—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ
    silence_streak = 0
    MAX_SILENCE_BEFORE_SKIP = 50  # ~1.5 —Å–µ–∫ —Ç–∏—à–∏–Ω—ã –ø–æ–¥—Ä—è–¥ = —Å–∫–∏–ø–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    
    # –°—á—ë—Ç—á–∏–∫ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏
    message_counter = 0
    
    try:
        await websocket.send(json.dumps({
            "type": "connected",
            "message": "ChatGPT-style STT server ready (v2.1 anti-hallucination)",
            "sample_rate": SAMPLE_RATE,
            "model": "whisper-small",
            "device": device,
            "features": [
                "adaptive_pause_detection",
                "streaming_partials",
                "speaker_identification",
                "hallucination_filter",
            ],
        }))
        
        async for message in websocket:
            try:
                data = json.loads(message)
                msg_type = data.get("type")
                
                if msg_type == "audio":
                    audio_b64 = data.get("audio") or ""
                    if not audio_b64:
                        continue
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–µ—Å—Å–∏–∏
                    session.last_activity = time.time()
                    
                    # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏ (–∫–∞–∂–¥—ã–µ 500 —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ —Å–µ—Å—Å–∏—é)
                    message_counter += 1
                    if message_counter % 500 == 0:
                        await cleanup_gpu_memory(force=True)
                    
                    try:
                        audio_chunk = np.frombuffer(
                            base64.b64decode(audio_b64),
                            dtype=np.int16
                        ).astype(np.float32) / 32768.0
                    except Exception as e:
                        print(f"‚ö†Ô∏è [{client_id}] Audio decode error: {e}")
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–Ω–µ—Ä–≥–∏—é —á–∞–Ω–∫–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç–∏—à–∏–Ω—ã
                    chunk_energy = calculate_energy(audio_chunk)
                    if chunk_energy < VADConfig.ENERGY_THRESHOLD:
                        silence_streak += 1
                    else:
                        silence_streak = 0
                    
                    # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ —Ç–∏—à–∏–Ω–∞ - —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
                    if silence_streak > MAX_SILENCE_BEFORE_SKIP:
                        if session.state != SpeechState.SILENCE:
                            print(f"üîá [{client_id}] Long silence detected, resetting buffers")
                            session.speech_buffer = []
                            session.state = SpeechState.SILENCE
                            session.speech_frames = 0
                            session.silence_frames = 0
                        pcm_buffer = np.array([], dtype=np.float32)
                        continue
                    
                    pcm_buffer = np.concatenate([pcm_buffer, audio_chunk])
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ—Ä–µ–π–º—ã
                    while len(pcm_buffer) >= frame_samples:
                        frame = pcm_buffer[:frame_samples]
                        pcm_buffer = pcm_buffer[frame_samples:]
                        
                        result = await process_vad_frame(session, frame, websocket)
                        if result:
                            try:
                                await websocket.send(json.dumps(result))
                            except websockets.exceptions.ConnectionClosed:
                                return  # –ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è
                
                elif msg_type == "finalize":
                    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
                    if session.state != SpeechState.SILENCE:
                        result = await finalize_segment(session)
                        try:
                            if result:
                                await websocket.send(json.dumps(result))
                            else:
                                await websocket.send(json.dumps({
                                    "type": "transcription",
                                    "text": "",
                                    "is_final": True,
                                    "timestamp": datetime.now().isoformat(),
                                }))
                        except websockets.exceptions.ConnectionClosed:
                            return  # –ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è
                    else:
                        if len(pcm_buffer) > frame_samples:
                            session.speech_buffer = [pcm_buffer]
                            result = await finalize_segment(session)
                            pcm_buffer = np.array([], dtype=np.float32)
                            if result:
                                await websocket.send(json.dumps(result))
                            else:
                                await websocket.send(json.dumps({
                                    "type": "transcription",
                                    "text": "",
                                    "is_final": True,
                                    "timestamp": datetime.now().isoformat(),
                                }))
                        else:
                            await websocket.send(json.dumps({
                                "type": "transcription",
                                "text": "",
                                "is_final": True,
                                "timestamp": datetime.now().isoformat(),
                            }))
                
                elif msg_type == "ping":
                    await websocket.send(json.dumps({
                        "type": "pong",
                        "timestamp": datetime.now().isoformat(),
                    }))
                
                elif msg_type == "reset":
                    session.conversation_context = []
                    session.last_transcript = ""
                    await websocket.send(json.dumps({
                        "type": "reset_ack",
                        "timestamp": datetime.now().isoformat(),
                    }))
                
                elif msg_type == "config":
                    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    if "pause_ms" in data:
                        VADConfig.DEFAULT_PAUSE_MS = int(data["pause_ms"])
                    if "energy_threshold" in data:
                        VADConfig.ENERGY_THRESHOLD = float(data["energy_threshold"])
                    await websocket.send(json.dumps({
                        "type": "config_ack",
                        "config": {
                            "pause_ms": VADConfig.DEFAULT_PAUSE_MS,
                            "energy_threshold": VADConfig.ENERGY_THRESHOLD,
                        },
                    }))
            
            except json.JSONDecodeError:
                await websocket.send(json.dumps({
                    "type": "error",
                    "message": "Invalid JSON",
                }))
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                import traceback
                traceback.print_exc()
                await websocket.send(json.dumps({
                    "type": "error",
                    "message": str(e),
                }))
    
    except websockets.exceptions.ConnectionClosed:
        print(f"üîå –ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è: {client_id}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ handle_client: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print(f"üëã –°–µ—Å—Å–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {client_id}")
        print(f"   üìä –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {session.total_segments}")
        print(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è —Ä–µ—á–∏: {session.total_speech_ms/1000:.1f}—Å")
        print(f"   üé≠ –°–ø–∏–∫–µ—Ä–æ–≤: {session.speaker_counter}")
        
        # –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä—ã —Å–µ—Å—Å–∏–∏
        session.audio_buffer.clear()
        session.speech_buffer.clear()
        session.conversation_context.clear()
        
        async with sessions_lock:
            if client_id in sessions:
                del sessions[client_id]
        
        # –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞
        await cleanup_gpu_memory()


async def main():
    """–ó–∞–ø—É—Å–∫ WebSocket —Å–µ—Ä–≤–µ—Ä–∞ —Å graceful shutdown"""
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown
    stop_event = asyncio.Event()
    
    def signal_handler():
        print("\nüõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è...")
        stop_event.set()
    
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤
    loop = asyncio.get_event_loop()
    for sig in (signal.SIGTERM, signal.SIGINT):
        try:
            loop.add_signal_handler(sig, signal_handler)
        except NotImplementedError:
            # Windows –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç add_signal_handler
            pass
    
    server = await websockets.serve(
        handle_client,
        "0.0.0.0",
        8765,
        ping_interval=20,
        ping_timeout=20,
        max_size=10 * 1024 * 1024,  # 10MB max message size
        close_timeout=10,  # –¢–∞–π–º–∞—É—Ç –∑–∞–∫—Ä—ã—Ç–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
    )
    
    print("üéß –û–∂–∏–¥–∞—é –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π...")
    
    # –§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏
    async def periodic_cleanup():
        while not stop_event.is_set():
            await asyncio.sleep(GPU_CLEANUP_INTERVAL)
            await cleanup_gpu_memory()
            # –õ–æ–≥–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–π
            async with sessions_lock:
                if sessions:
                    print(f"üìä –ê–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π: {len(sessions)}")
    
    cleanup_task = asyncio.create_task(periodic_cleanup())
    
    try:
        await stop_event.wait()
    except asyncio.CancelledError:
        pass
    finally:
        print("üîÑ –ó–∞–≤–µ—Ä—à–∞—é —Å–µ—Ä–≤–µ—Ä...")
        cleanup_task.cancel()
        server.close()
        await server.wait_closed()
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º executor
        whisper_executor.shutdown(wait=True, cancel_futures=True)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        print("‚úÖ –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nüëã –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ Ctrl+C")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        print("üßπ –†–µ—Å—É—Ä—Å—ã –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω—ã")
