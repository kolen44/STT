"""
WebSocket STT —Å–µ—Ä–≤–µ—Ä v2.0 - ChatGPT-style –¥–∏–∞–ª–æ–≥–æ–≤—ã–π —Ä–µ–∂–∏–º
OpenAI Whisper Small –Ω–∞ GPU

–û—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—É–∑ (–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ 0.5-1.5 —Å–µ–∫ –≤–º–µ—Å—Ç–æ 6 —Å–µ–∫)
- Streaming partial results –≤–æ –≤—Ä–µ–º—è —Ä–µ—á–∏
- Proper sentence boundary detection
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö "Kiko"
- Real-time feedback –∫–∞–∫ –≤ ChatGPT
"""
import warnings
warnings.filterwarnings("ignore")

import asyncio
import websockets
import json
import whisper
import torch
import numpy as np
from datetime import datetime
import base64
import time
from collections import defaultdict
import re
from difflib import get_close_matches
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass, field
from enum import Enum
import threading

# ============ –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ============
print("=" * 80)
print("üöÄ WEBSOCKET STT SUPER SERVER v2.0 (ChatGPT-style)")
print("=" * 80)

# –§–æ—Ä—Å–∏—Ä—É–µ–º GPU —Ä–µ–∂–∏–º
if not torch.cuda.is_available():
    print("‚ùå CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ CUDA Toolkit –∏ GPU –¥—Ä–∞–π–≤–µ—Ä—ã")
    print("   https://developer.nvidia.com/cuda-downloads")
    import sys
    sys.exit(1)

device = "cuda"
print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º Whisper Small
print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º Whisper Small ({device.upper()})...")
start_time = time.time()
whisper_model = whisper.load_model("small", device=device)
load_time = time.time() - start_time
print(f"‚úÖ Whisper –∑–∞–≥—Ä—É–∂–µ–Ω –∑–∞ {load_time:.2f}—Å\n")

print("=" * 80)
print(f"üåê WebSocket —Å–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –Ω–∞ ws://0.0.0.0:8765")
print(f"üìä –†–µ–∂–∏–º: {device.upper()} | ChatGPT-style –¥–∏–∞–ª–æ–≥")
print("=" * 80)
print()

# ===============================
# –ù–ê–°–¢–†–û–ô–ö–ò - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –î–õ–Ø –î–ò–ê–õ–û–ì–ê
# ===============================
SAMPLE_RATE = 16000
BYTES_PER_SAMPLE = 2  # int16


# === VAD –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ - –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–û –î–õ–Ø –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –°–ö–û–†–û–°–¢–ò ===
class VADConfig:
    # –ü–æ—Ä–æ–≥ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏
    ENERGY_THRESHOLD = 0.010  # –°–Ω–∏–∂–µ–Ω –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ (–∑–∞—â–∏—Ç–∞ –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π)
    MIN_AUDIO_ENERGY = 0.012  # –°–Ω–∏–∂–µ–Ω
    
    # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—É–∑—ã - –£–í–ï–õ–ò–ß–ï–ù–´ —á—Ç–æ–±—ã –Ω–µ –æ–±—Ä–µ–∑–∞—Ç—å —Ä–µ—á—å
    MIN_PAUSE_MS = 600        # 600–º—Å –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–∑ (–±—ã–ª–æ 400)
    DEFAULT_PAUSE_MS = 800    # 800–º—Å —Å—Ç–∞–Ω–¥–∞—Ä—Ç (–±—ã–ª–æ 550)
    MAX_PAUSE_MS = 1200       # 1200–º—Å –º–∞–∫—Å –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–±—ã–ª–æ 800)
    QUESTION_PAUSE_MS = 650   # 650–º—Å –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ (–±—ã–ª–æ 450)
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏
    MIN_SPEECH_MS = 150       # 150–º—Å - –±—ã—Å—Ç—Ä–µ–µ —Ä–µ–∞–≥–∏—Ä—É–µ–º
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞
    MAX_SEGMENT_MS = 60000    # 60 —Å–µ–∫—É–Ω–¥
    
    # –ü–æ—Ä–æ–≥ –¥–ª—è –º—è–≥–∫–æ–π —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    SOFT_SEGMENT_MS = 25000   # 25 —Å–µ–∫
    
    # –ß–∞—Å—Ç–æ—Ç–∞ partial - –£–í–ï–õ–ò–ß–ï–ù–ê –¥–ª—è real-time
    PARTIAL_INTERVAL_MS = 200  # –ö–∞–∂–¥—ã–µ 200–º—Å - –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ!
    
    # –†–∞–∑–º–µ—Ä VAD —Ñ—Ä–µ–π–º–∞
    FRAME_MS = 20             # 20–º—Å —Ñ—Ä–µ–π–º—ã - –±—ã—Å—Ç—Ä–µ–µ!
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–µ–π–º–æ–≤ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏
    SPEECH_START_FRAMES = 2   # 2 —Ñ—Ä–µ–π–º–∞ = 40–º—Å –¥–ª—è —Å—Ç–∞—Ä—Ç–∞
    
    # –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏
    DEDUP_WINDOW_MS = 3000    # 3 —Å–µ–∫—É–Ω–¥—ã


# === Hotwords –¥–ª—è boosting ===
HOTWORDS = ["Kiko", "kiko", "KIKO", "–∫–∏–∫–æ", "–∫—ñ–∫–æ", "–ö–∏–∫–æ"]

# Initial prompt - –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π!
# –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º "assistant", "voice assistant" –∏ —Ç.–¥. - Whisper –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –∏—Ö!
INITIAL_PROMPT = None  # –û—Ç–∫–ª—é—á—ë–Ω –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è post-correction - –ú–ù–û–ì–û –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
CORRECTION_DICT = {
    # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã (–≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–∏–∑–Ω–æ—à–µ–Ω–∏—è)
    "kiko": "Kiko", "kyko": "Kiko", "keeko": "Kiko", "kico": "Kiko",
    "kieko": "Kiko", "keyko": "Kiko", "tico": "Kiko", "tiko": "Kiko",
    "keco": "Kiko", "cico": "Kiko", "qico": "Kiko", "kika": "Kiko",
    "kikko": "Kiko", "keko": "Kiko", "chico": "Kiko", "kiku": "Kiko",
    "keeco": "Kiko", "kigo": "Kiko", "kijo": "Kiko", "kito": "Kiko",
    "kikou": "Kiko", "kicco": "Kiko", "kyco": "Kiko", "kiko's": "Kiko",
    "quico": "Kiko", "keko": "Kiko", "keekou": "Kiko", "kikov": "Kiko",
    "kiko—É": "Kiko", "keiko": "Kiko", "kico": "Kiko", "kykou": "Kiko",
    # Whisper —á–∞—Å—Ç–æ —Å–ª—ã—à–∏—Ç –∫–∞–∫ "key call" –∏–ª–∏ "he called"
    "keycall": "Kiko", "key-call": "Kiko",
    # –†—É—Å—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    "–∫–∏–∫–æ": "Kiko", "–∫—ñ–∫–æ": "Kiko", "–∫–∏–∫–∞": "Kiko", "–∫–µ–∫–æ": "Kiko", "—Ç–∏–∫–æ": "Kiko",
    "–∫–∏–∫—É": "Kiko", "–∫—ñ–∫—É": "Kiko", "–∫–∏–∫–æ.": "Kiko", "–∫–∏–∫–æ,": "Kiko",
    "–∫–∏–≥–æ": "Kiko", "–∫—ñ–π–∫–æ": "Kiko", "–∫–∏–π–∫–æ": "Kiko",
}

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ —Ñ—Ä–∞–∑—ã
QUESTION_PATTERNS = [
    r'\?$',
    r'^(what|who|where|when|why|how|can|could|would|should|is|are|do|does|did)\b',
    r'^(—á—Ç–æ|–∫—Ç–æ|–≥–¥–µ|–∫–æ–≥–¥–∞|–ø–æ—á–µ–º—É|–∫–∞–∫|–º–æ–∂–Ω–æ|–º–æ–≥—É)\b',
]

COMMAND_PATTERNS = [
    r'^(play|stop|pause|next|previous|volume|mute|unmute)\b',
    r'^(–≤–∫–ª—é—á–∏|–≤—ã–∫–ª—é—á–∏|–ø–æ—Å—Ç–∞–≤—å|—Å–ª–µ–¥—É—é—â|–ø—Ä–µ–¥—ã–¥—É—â|–≥—Ä–æ–º–∫–æ—Å—Ç—å)\b',
    r'^(search|find|show|open|close|start|turn)\b',
    r'^(–Ω–∞–π–¥–∏|–ø–æ–∫–∞–∂–∏|–æ—Ç–∫—Ä–æ–π|–∑–∞–∫—Ä–æ–π|–∑–∞–ø—É—Å—Ç–∏)\b',
]

SHORT_RESPONSE_PATTERNS = [
    r'^(yes|no|ok|okay|sure|yeah|yep|nope|maybe)$',
    r'^(–¥–∞|–Ω–µ—Ç|–æ–∫–µ–π|–ª–∞–¥–Ω–æ|—Ö–æ—Ä–æ—à–æ|–º–æ–∂–µ—Ç)$',
]


# ===============================
# –°–û–°–¢–û–Ø–ù–ò–ï –ö–õ–ò–ï–ù–¢–ê
# ===============================
class SpeechState(Enum):
    SILENCE = 0      # –¢–∏—à–∏–Ω–∞, –æ–∂–∏–¥–∞–Ω–∏–µ
    SPEECH = 1       # –ê–∫—Ç–∏–≤–Ω–∞—è —Ä–µ—á—å
    PAUSE = 2        # –ü–∞—É–∑–∞ –≤ —Ä–µ—á–∏ (–º–æ–∂–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å—Å—è)


@dataclass
class ClientSession:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π —Å–µ—Å—Å–∏–∏"""
    client_id: str
    
    # –ê—É–¥–∏–æ –±—É—Ñ–µ—Ä—ã
    audio_buffer: List[np.ndarray] = field(default_factory=list)
    speech_buffer: List[np.ndarray] = field(default_factory=list)
    
    # –°–æ—Å—Ç–æ—è–Ω–∏–µ VAD
    state: SpeechState = SpeechState.SILENCE
    speech_frames: int = 0
    silence_frames: int = 0
    
    # Timing
    speech_start_time: float = 0.0
    last_partial_time: float = 0.0
    pause_start_time: float = 0.0
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –ø–∞—É–∑
    last_transcript: str = ""
    conversation_context: List[str] = field(default_factory=list)
    
    # Speaker tracking
    speaker_sessions: Dict[str, int] = field(default_factory=dict)
    speaker_counter: int = 0
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    total_speech_ms: float = 0.0
    total_segments: int = 0
    
    # –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–æ–≤—Ç–æ—Ä–Ω—É—é –æ—Ç–ø—Ä–∞–≤–∫—É –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    last_sent_text: str = ""
    last_sent_time: float = 0.0


# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–µ—Å—Å–∏–π
sessions: Dict[str, ClientSession] = {}
sessions_lock = threading.Lock()


# ===============================
# –£–¢–ò–õ–ò–¢–´
# ===============================

def calculate_energy(audio: np.ndarray) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç RMS —ç–Ω–µ—Ä–≥–∏—é –∞—É–¥–∏–æ"""
    if len(audio) == 0:
        return 0.0
    return float(np.sqrt(np.mean(audio ** 2)))


def calculate_zero_crossings(audio: np.ndarray) -> int:
    """–ü–æ–¥—Å—á—ë—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –Ω—É–ª—è"""
    if len(audio) < 2:
        return 0
    return int(np.sum(np.abs(np.diff(np.sign(audio))) > 0))


def is_speech_frame(audio: np.ndarray) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ—Ä–µ–π–º —Ä–µ—á—å—é"""
    if len(audio) == 0:
        return False
    
    energy = calculate_energy(audio)
    if energy < VADConfig.ENERGY_THRESHOLD:
        return False
    
    # –®—É–º –∏–º–µ–µ—Ç –º–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –Ω—É–ª—è, —Ä–µ—á—å - –º–µ–Ω—å—à–µ
    zc_rate = calculate_zero_crossings(audio) / len(audio) if len(audio) > 0 else 0
    if zc_rate > 0.5:
        return False
    
    return True


def determine_pause_duration(text: str, speech_duration_ms: float) -> int:
    """
    –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–û–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—É–∑—ã - –∫–∞–∫ —É ChatGPT Voice.
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–∫–ª–∏–∫–∞.
    """
    text_lower = text.lower().strip()
    words = text_lower.split()
    word_count = len(words)
    
    # 1. –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã (1-2 —Å–ª–æ–≤–∞) - –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞
    if word_count <= 2:
        return VADConfig.MIN_PAUSE_MS
    
    # 2. –ö–æ–º–∞–Ω–¥—ã —Å wake word - –±—ã—Å—Ç—Ä–∞—è —Ä–µ–∞–∫—Ü–∏—è
    has_kiko = any(w in text_lower for w in ['kiko', '–∫–∏–∫–æ'])
    if has_kiko and word_count <= 5:
        return VADConfig.MIN_PAUSE_MS
    
    # 3. –Ø–≤–Ω–æ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Ç–æ—á–∫–∞, !, ?)
    if re.search(r'[.!?]$', text_lower):
        return VADConfig.MIN_PAUSE_MS + 50  # –ù–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ –¥–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    
    # 4. –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã - –±—ã—Å—Ç—Ä–æ
    for pattern in SHORT_RESPONSE_PATTERNS:
        if re.match(pattern, text_lower, re.IGNORECASE):
            return VADConfig.MIN_PAUSE_MS
    
    # 5. –ö–æ–º–∞–Ω–¥—ã - –±—ã—Å—Ç—Ä–æ
    for pattern in COMMAND_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            return VADConfig.MIN_PAUSE_MS + 50
    
    # 6. –í–æ–ø—Ä–æ—Å—ã –±–µ–∑ –∑–Ω–∞–∫–∞ –≤–æ–ø—Ä–æ—Å–∞ –≤ –∫–æ–Ω—Ü–µ
    for pattern in QUESTION_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            return VADConfig.QUESTION_PAUSE_MS
    
    # 7. –ù–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è - –∂–¥—ë–º –¥–æ–ª—å—à–µ
    # –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ—Å—Ç–∏:
    #   - –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ —Å–æ—é–∑/–ø—Ä–µ–¥–ª–æ–≥
    #   - –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –∑–∞–ø—è—Ç—É—é
    #   - –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ < 3 –±—É–∫–≤ –∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
    incomplete_endings = ['–∏', '–∞', '–Ω–æ', '–∏–ª–∏', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', 
                         'and', 'or', 'but', 'that', 'which', 'who', 'where',
                         'the', 'a', 'an', 'to', 'for', 'with', 'in', 'on']
    
    if words and words[-1] in incomplete_endings:
        return VADConfig.MAX_PAUSE_MS
    
    if text_lower.endswith(','):
        return VADConfig.MAX_PAUSE_MS
    
    # 8. –ü–æ –¥–ª–∏–Ω–µ —Ä–µ—á–∏ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤
    if word_count <= 4:
        return VADConfig.MIN_PAUSE_MS + 100
    elif word_count <= 8:
        return VADConfig.DEFAULT_PAUSE_MS
    else:
        # –î–ª–∏–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ - –≥–æ—Ç–æ–≤–æ
        if re.search(r'[.!?;]$', text_lower):
            return VADConfig.DEFAULT_PAUSE_MS
        return VADConfig.MAX_PAUSE_MS


# –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã Kiko –¥–ª—è fuzzy matching
KIKO_PHONETIC_VARIANTS = [
    "kiko", "kyko", "keeko", "kico", "kieko", "keyko", "tico", "tiko",
    "keco", "cico", "qico", "kika", "kikko", "keko", "chico", "kiku",
    "keeco", "kigo", "kijo", "kito", "kikou", "kicco", "kyco", "keiko",
    "quico", "keekou", "kikov", "key co", "key go", "ki ko", "ki go",
    "kee ko", "kee go", "key cool", "kekko", "geko", "gecko",
]


def fuzzy_match_kiko(word: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂–µ –ª–∏ —Å–ª–æ–≤–æ –Ω–∞ 'Kiko' (fuzzy matching)"""
    clean = re.sub(r'[^\w]', '', word).lower()
    
    # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    if clean in CORRECTION_DICT:
        return True
    
    # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    if clean in KIKO_PHONETIC_VARIANTS:
        return True
    
    # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ (–ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è)
    if len(clean) >= 3 and len(clean) <= 6:
        for variant in ["kiko", "keko", "kico", "kiku"]:
            diff = sum(1 for a, b in zip(clean, variant) if a != b)
            diff += abs(len(clean) - len(variant))
            if diff <= 1:  # –ú–∞–∫—Å–∏–º—É–º 1 –æ—à–∏–±–∫–∞
                return True
    
    return False


def apply_post_correction(text: str) -> str:
    """
    –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Å—Ç-–∫–æ—Ä—Ä–µ–∫—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ —Å –ê–ì–†–ï–°–°–ò–í–ù–´–ú –ø–æ–∏—Å–∫–æ–º Kiko.
    –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ –∏ –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ Kiko.
    """
    if not text:
        return text
    
    words = text.split()
    corrected_words = []
    
    for word in words:
        clean_word = re.sub(r'[^\w\s]', '', word).lower()
        punctuation_after = re.sub(r'^[\w\s]+', '', word)
        punctuation_before = re.sub(r'[\w\s]+$', '', word)
        
        corrected = None
        
        # 1. –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä–µ
        if clean_word in CORRECTION_DICT:
            corrected = CORRECTION_DICT[clean_word]
        # 2. Fuzzy matching –¥–ª—è "–ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ Kiko" —Å–ª–æ–≤
        elif fuzzy_match_kiko(word):
            corrected = "Kiko"
        # 3. –ë–ª–∏–∑–∫–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —á–µ—Ä–µ–∑ difflib
        elif len(clean_word) > 2:
            matches = get_close_matches(clean_word, CORRECTION_DICT.keys(), n=1, cutoff=0.70)  # –°–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥
            if matches:
                corrected = CORRECTION_DICT[matches[0]]
        
        if corrected:
            final_word = punctuation_before + corrected + punctuation_after
            corrected_words.append(final_word)
        else:
            corrected_words.append(word)
    
    result = ' '.join(corrected_words)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã Kiko —Ä—è–¥–æ–º: "Kiko Kiko –≤–∫–ª—é—á–∏" -> "Kiko, –≤–∫–ª—é—á–∏"
    result = re.sub(r'\bKiko\s+Kiko\b', 'Kiko,', result, flags=re.IGNORECASE)
    
    # –£–±–∏—Ä–∞–µ–º "Kiko assistant" –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
    result = re.sub(r'\bKiko\s+assistant\b', 'Kiko', result, flags=re.IGNORECASE)
    
    return result


def get_speaker_hash(audio_data: np.ndarray) -> str:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ø–∏–∫–µ—Ä–∞"""
    if len(audio_data) == 0:
        return "unknown"
    
    mean_amplitude = np.mean(np.abs(audio_data))
    std_amplitude = np.std(audio_data)
    zero_crossings = calculate_zero_crossings(audio_data)
    
    fft = np.fft.rfft(audio_data)
    magnitude = np.abs(fft)
    
    spectral_centroid = 0
    if np.sum(magnitude) > 0:
        spectral_centroid = np.sum(magnitude * np.arange(len(magnitude))) / np.sum(magnitude)
    
    n = len(magnitude)
    low_freq = np.sum(magnitude[:n//4]) if n >= 4 else 0
    high_freq = np.sum(magnitude[3*n//4:]) if n >= 4 else 0
    
    return f"{mean_amplitude:.5f}_{std_amplitude:.5f}_{zero_crossings}_{spectral_centroid:.2f}_{low_freq:.2f}_{high_freq:.2f}"


# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π Whisper (—Ç–µ–∫—Å—Ç –∏–∑ –ø—Ä–æ–º–ø—Ç–∞ –∏–ª–∏ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è)
HALLUCINATION_PATTERNS = [
    r'^kiko[\s,\.]*kiko[\s,\.]*kiko',  # –ü–æ–≤—Ç–æ—Ä—è—é—â–µ–µ—Å—è Kiko
    r'^(kiko[\s,\.]*){{3,}}',  # Kiko 3+ —Ä–∞–∑ –ø–æ–¥—Ä—è–¥
    r'^–∫–∏–∫–æ[\s,\.]*–∫–∏–∫–æ[\s,\.]*–∫–∏–∫–æ',  # –¢–æ –∂–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    r'voice assistant',  # –ì–õ–ê–í–ù–´–ô –∏—Å—Ç–æ—á–Ω–∏–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π!
    r'kiko assistant',   # –ß–∞—Å—Ç–∞—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è
    r'kiko is a',        # –ì–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
    r'assistant kiko',   # –ï—â—ë –≤–∞—Ä–∏–∞–Ω—Ç
    r'common phrases',   # –ò–∑ –ø—Ä–æ–º–ø—Ç–∞  
    r'having a conversation',  # –ò–∑ –ø—Ä–æ–º–ø—Ç–∞
    r'^\s*\.+\s*$',     # –¢–æ–ª—å–∫–æ —Ç–æ—á–∫–∏
    r'^\s*,+\s*$',      # –¢–æ–ª—å–∫–æ –∑–∞–ø—è—Ç—ã–µ
    r'thank you for watching',  # –¢–∏–ø–∏—á–Ω–∞—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è YouTube
    r'thanks for watching',
    r'subscribe',
    r'like and subscribe',
    r'please subscribe',
    r'^kiko\.?$',        # –ü—Ä–æ—Å—Ç–æ "Kiko" –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—á–∞—Å—Ç–æ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è)
    r'^kiko,?\.?$',     # "Kiko," –∏–ª–∏ "Kiko."
    r'\bthe\s+kiko\b',  # "the Kiko" - –Ω–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ
]

def is_noise_or_garbage(text: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —à—É–º–æ–º –∏–ª–∏ –º—É—Å–æ—Ä–æ–º"""
    if not text:
        return True
    
    t = text.strip()
    if len(t) < 2:
        return True
    if re.match(r'^\[[^\]]+\]$', t):  # [BLANK_AUDIO], [MUSIC], etc.
        return True
    if re.match(r'^[\s\.,!?\-\‚Äî\‚Äì\'\"‚Ä¶]+$', t):
        return True
    if re.match(r'^(.)\1{2,}$', t):
        return True
    
    return False


def is_hallucination(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–µ–π Whisper"""
    if not text:
        return False
    
    t = text.lower().strip()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    for pattern in HALLUCINATION_PATTERNS:
        if re.search(pattern, t, re.IGNORECASE):
            return True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞ ("kiko kiko kiko" –∏–ª–∏ "the the the")
    words = t.split()
    if len(words) >= 3:
        # –ï—Å–ª–∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è 3+ —Ä–∞–∑–∞ –ø–æ–¥—Ä—è–¥
        for i in range(len(words) - 2):
            if words[i] == words[i+1] == words[i+2]:
                return True
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ Kiko (–±–æ–ª—å—à–µ 2 –≤ –∫–æ—Ä–æ—Ç–∫–æ–º —Ç–µ–∫—Å—Ç–µ)
    kiko_count = len(re.findall(r'\bkiko\b', t, re.IGNORECASE))
    word_count = len(words)
    if word_count > 0 and kiko_count > 2 and kiko_count / word_count > 0.5:
        return True
    
    return False


def has_sufficient_audio_energy(audio: np.ndarray) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –≤ –∞—É–¥–∏–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è —Ä–µ—á–∏"""
    if len(audio) == 0:
        return False
    
    energy = calculate_energy(audio)
    
    # –ï—Å–ª–∏ —Å—Ä–µ–¥–Ω—è—è —ç–Ω–µ—Ä–≥–∏—è —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è - —ç—Ç–æ —Ç–∏—à–∏–Ω–∞
    if energy < VADConfig.MIN_AUDIO_ENERGY:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –∫–∞–∫–∏–µ-—Ç–æ "–ø–∏–∫–∏" (—Ä–µ—á—å –∏–º–µ–µ—Ç –¥–∏–Ω–∞–º–∏–∫—É)
    max_amplitude = np.max(np.abs(audio))
    if max_amplitude < 0.05:  # –ï—Å–ª–∏ –º–∞–∫—Å –∞–º–ø–ª–∏—Ç—É–¥–∞ < 5% - —ç—Ç–æ —Ç–∏—à–∏–Ω–∞/—à—É–º
        return False
    
    return True


# ===============================
# –ì–õ–ê–í–ù–ê–Ø –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò
# ===============================

async def transcribe_audio(audio: np.ndarray, session: ClientSession) -> Tuple[str, dict]:
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –∑–∞—â–∏—Ç–æ–π –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."""
    audio_duration = len(audio) / SAMPLE_RATE
    
    # –ó–ê–©–ò–¢–ê –û–¢ –ì–ê–õ–õ–Æ–¶–ò–ù–ê–¶–ò–ô: –ø—Ä–æ–≤–µ—Ä—è–µ–º —ç–Ω–µ—Ä–≥–∏—é –∞—É–¥–∏–æ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π
    if not has_sufficient_audio_energy(audio):
        print(f"‚ö†Ô∏è [{session.client_id}] Audio energy too low, skipping transcription")
        return "", {"transcription_time_ms": 0, "audio_duration_s": round(audio_duration, 3), 
                   "realtime_factor": 0, "samples": len(audio), "skipped": "low_energy"}
    
    # Noise gate
    audio = audio * (np.abs(audio) > 0.008)  # –£–≤–µ–ª–∏—á–µ–Ω –ø–æ—Ä–æ–≥
    
    start_time = time.perf_counter()
    
    # –ö–æ–Ω—Ç–µ–∫—Å—Ç - –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô! "assistant" –≤—ã–∑—ã–≤–∞–µ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–ø–ª–∏–∫–∏ –ë–ï–ó —Å–ª–æ–≤–∞ "assistant"
    context_prompt = None  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–µ–∑ –ø—Ä–æ–º–ø—Ç–∞
    if session.conversation_context:
        recent = session.conversation_context[-2:]  # –¢–æ–ª—å–∫–æ 2 –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ—Ä–∞–∑—ã
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–æ–≤–æ "assistant" –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        clean_context = ' '.join(recent).replace('assistant', '').replace('Assistant', '')
        if clean_context.strip():
            context_prompt = clean_context.strip()
    
    result = whisper_model.transcribe(
        audio,
        language="en",
        initial_prompt=context_prompt,
        fp16=True,
        condition_on_previous_text=False,  # –û–¢–ö–õ–Æ–ß–ï–ù–û –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
        no_speech_threshold=0.6,  # –£–≤–µ–ª–∏—á–µ–Ω –ø–æ—Ä–æ–≥ "–Ω–µ—Ç —Ä–µ—á–∏"
        logprob_threshold=-0.8,   # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    )
    
    text = result["text"].strip()
    text = apply_post_correction(text)
    
    end_time = time.perf_counter()
    transcription_time = (end_time - start_time) * 1000
    rtf = audio_duration / (transcription_time / 1000) if transcription_time > 0 else 0
    
    # –ü–†–û–í–ï–†–ö–ê –ù–ê –ì–ê–õ–õ–Æ–¶–ò–ù–ê–¶–ò–ò
    if is_hallucination(text):
        print(f"üö´ [{session.client_id}] Hallucination filtered: {text!r}")
        return "", {"transcription_time_ms": round(transcription_time, 2), 
                   "audio_duration_s": round(audio_duration, 3),
                   "realtime_factor": round(rtf, 2), "samples": len(audio), 
                   "filtered": "hallucination", "original_text": text}
    
    metrics = {
        "transcription_time_ms": round(transcription_time, 2),
        "audio_duration_s": round(audio_duration, 3),
        "realtime_factor": round(rtf, 2),
        "samples": len(audio),
    }
    
    return text, metrics


async def process_vad_frame(session: ClientSession, frame: np.ndarray, websocket) -> Optional[dict]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω VAD —Ñ—Ä–µ–π–º. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∏–ª–∏ None."""
    is_speech = is_speech_frame(frame)
    current_time = time.time()
    
    result = None
    
    if is_speech:
        session.speech_frames += 1
        session.silence_frames = 0
        
        if session.state == SpeechState.SILENCE:
            if session.speech_frames >= VADConfig.SPEECH_START_FRAMES:
                session.state = SpeechState.SPEECH
                session.speech_start_time = current_time
                session.speech_buffer = list(session.audio_buffer[-5:])  # Preroll
                print(f"üé§ [{session.client_id}] Speech started")
        
        elif session.state == SpeechState.PAUSE:
            session.state = SpeechState.SPEECH
            print(f"üé§ [{session.client_id}] Speech resumed")
        
        if session.state == SpeechState.SPEECH:
            session.speech_buffer.append(frame)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º partial —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if current_time - session.last_partial_time > VADConfig.PARTIAL_INTERVAL_MS / 1000:
                session.last_partial_time = current_time
                
                if len(session.speech_buffer) > 0:
                    audio = np.concatenate(session.speech_buffer)
                    # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–æ 200–º—Å –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã—Ö partial
                    if len(audio) > SAMPLE_RATE * 0.2:
                        text, _ = await transcribe_audio(audio, session)
                        if text and not is_noise_or_garbage(text):
                            session.last_transcript = text
                            result = {
                                "type": "partial",
                                "text": text,
                                "is_final": False,
                                "timestamp": datetime.now().isoformat(),
                            }
    else:
        session.silence_frames += 1
        session.speech_frames = max(0, session.speech_frames - 1)
        
        if session.state == SpeechState.SPEECH:
            session.speech_buffer.append(frame)
            
            if session.silence_frames >= 2:  # ~40–º—Å —Ç–∏—à–∏–Ω—ã -> –ø–µ—Ä–µ—Ö–æ–¥ –≤ –ø–∞—É–∑—É (–±—ã—Å—Ç—Ä–µ–µ!)
                session.state = SpeechState.PAUSE
                session.pause_start_time = current_time
        
        elif session.state == SpeechState.PAUSE:
            session.speech_buffer.append(frame)
            
            speech_duration_ms = (current_time - session.speech_start_time) * 1000
            pause_duration_ms = session.silence_frames * VADConfig.FRAME_MS
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –ø–∞—É–∑—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞
            required_pause = VADConfig.DEFAULT_PAUSE_MS
            if session.last_transcript:
                required_pause = determine_pause_duration(session.last_transcript, speech_duration_ms)
            
            # –§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –µ—Å–ª–∏ –ø–∞—É–∑–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è
            if pause_duration_ms >= required_pause:
                result = await finalize_segment(session)
            
            # CONTINUOUS MODE: –µ—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç –¥–ª–∏–Ω–Ω—ã–π –∏ –µ—Å—Ç—å –∫–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ - —Ä–∞–∑–±–∏–≤–∞–µ–º
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å —á–∞—Å—Ç—è–º–∏ –¥–ª–∏–Ω–Ω—ã–µ –º–æ–Ω–æ–ª–æ–≥–∏ –±–µ–∑ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è
            elif speech_duration_ms > VADConfig.SOFT_SEGMENT_MS and pause_duration_ms >= 400:
                print(f"üì§ [{session.client_id}] Soft split at {speech_duration_ms:.0f}ms (continuous mode)")
                result = await finalize_segment(session, continue_listening=True)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ MAX - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ MAX
    max_samples = int(VADConfig.MAX_SEGMENT_MS * SAMPLE_RATE / 1000)
    if session.state in (SpeechState.SPEECH, SpeechState.PAUSE):
        if sum(len(b) for b in session.speech_buffer) > max_samples:
            print(f"üì§ [{session.client_id}] Hard split at {VADConfig.MAX_SEGMENT_MS}ms (continuous mode)")
            result = await finalize_segment(session, continue_listening=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—Ä–µ–π–º –¥–ª—è preroll
    session.audio_buffer.append(frame)
    if len(session.audio_buffer) > 10:
        session.audio_buffer.pop(0)
    
    return result


async def finalize_segment(session: ClientSession, continue_listening: bool = False) -> Optional[dict]:
    """–§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç —Ä–µ—á–∏
    
    Args:
        session: –ö–ª–∏–µ–Ω—Ç—Å–∫–∞—è —Å–µ—Å—Å–∏—è
        continue_listening: –ï—Å–ª–∏ True - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª—É—à–∞—Ç—å –ø–æ—Å–ª–µ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ (continuous mode)
    """
    if not session.speech_buffer:
        return None
    
    audio = np.concatenate(session.speech_buffer)
    duration_ms = len(audio) / SAMPLE_RATE * 1000
    
    # –°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è
    session.speech_buffer = []
    session.speech_frames = 0
    session.silence_frames = 0
    
    # CONTINUOUS MODE: –æ—Å—Ç–∞—ë–º—Å—è –≤ —Ä–µ–∂–∏–º–µ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if continue_listening:
        session.state = SpeechState.SPEECH
        session.speech_start_time = time.time()
        print(f"üîÑ [{session.client_id}] Continuing to listen after segment...")
    else:
        session.state = SpeechState.SILENCE
    
    if duration_ms < VADConfig.MIN_SPEECH_MS:
        print(f"‚è≠Ô∏è [{session.client_id}] Segment too short ({duration_ms:.0f}ms), skipping")
        return None
    
    # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê —ç–Ω–µ—Ä–≥–∏–∏ –ø–µ—Ä–µ–¥ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π
    if not has_sufficient_audio_energy(audio):
        print(f"‚ö†Ô∏è [{session.client_id}] Audio energy too low ({duration_ms:.0f}ms), skipping")
        return None
    
    # –°–ø–∏–∫–µ—Ä
    speaker_hash = get_speaker_hash(audio)
    if speaker_hash not in session.speaker_sessions:
        session.speaker_counter += 1
        session.speaker_sessions[speaker_hash] = session.speaker_counter
    speaker_num = session.speaker_sessions[speaker_hash]
    
    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
    text, metrics = await transcribe_audio(audio, session)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–ª–∏ –º—É—Å–æ—Ä
    if not text or is_noise_or_garbage(text):
        print(f"üóëÔ∏è [{session.client_id}] Empty or garbage filtered: {text!r}")
        return None
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
    if is_hallucination(text):
        print(f"üö´ [{session.client_id}] Hallucination in final: {text!r}")
        return None
    
    # –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –ª–∏ –º—ã —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –Ω–µ–¥–∞–≤–Ω–æ
    current_time = time.time()
    text_normalized = text.lower().strip()
    last_normalized = session.last_sent_text.lower().strip() if session.last_sent_text else ""
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–π —Ç–µ–∫—Å—Ç
    if last_normalized and text_normalized:
        time_since_last = (current_time - session.last_sent_time) * 1000
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∏–¥–µ–Ω—Ç–∏—á–µ–Ω –∏ –ø—Ä–æ—à–ª–æ –º–µ–Ω—å—à–µ DEDUP_WINDOW_MS
        if text_normalized == last_normalized and time_since_last < VADConfig.DEDUP_WINDOW_MS:
            print(f"üîÅ [{session.client_id}] Duplicate skipped: {text!r} (sent {time_since_last:.0f}ms ago)")
            return None
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂ (–æ–¥–∏–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –¥—Ä—É–≥–æ–π) –∏ –ø—Ä–æ—à–ª–æ –º–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
        if time_since_last < VADConfig.DEDUP_WINDOW_MS:
            if text_normalized in last_normalized or last_normalized in text_normalized:
                # –ï—Å–ª–∏ –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—á–µ –∏–ª–∏ —Ä–∞–≤–µ–Ω - –¥—É–±–ª–∏–∫–∞—Ç
                if len(text_normalized) <= len(last_normalized):
                    print(f"üîÅ [{session.client_id}] Partial duplicate skipped: {text!r}")
                    return None
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    session.last_sent_text = text
    session.last_sent_time = current_time
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    session.last_transcript = text
    session.conversation_context.append(text)
    if len(session.conversation_context) > 10:
        session.conversation_context.pop(0)
    
    session.total_speech_ms += duration_ms
    session.total_segments += 1
    
    print(f"üìù [{session.client_id}] Speaker #{speaker_num}: {text!r}")
    print(f"‚è±Ô∏è  Duration: {duration_ms:.0f}ms | Transcription: {metrics['transcription_time_ms']:.0f}ms | RTF: {metrics['realtime_factor']:.1f}x")
    
    return {
        "type": "transcription",
        "text": text,
        "is_final": True,
        "timestamp": datetime.now().isoformat(),
        "speaker_number": speaker_num,
        "metrics": metrics,
    }


# ===============================
# WS HANDLER
# ===============================

async def handle_client(websocket):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞"""
    client_id = str(id(websocket))
    
    # –°–æ–∑–¥–∞—ë–º —á–∏—Å—Ç—É—é —Å–µ—Å—Å–∏—é (–∑–∞—â–∏—Ç–∞ –æ—Ç –ø—Ä–æ–±–ª–µ–º –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ)
    session = ClientSession(client_id=client_id)
    with sessions_lock:
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é —Å–µ—Å—Å–∏—é –µ—Å–ª–∏ –±—ã–ª–∞ (–ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏)
        if client_id in sessions:
            print(f"‚ôªÔ∏è [{client_id}] Cleaning up previous session")
            del sessions[client_id]
        sessions[client_id] = session
    
    print(f"üîå –ö–ª–∏–µ–Ω—Ç –ø–æ–¥–∫–ª—é—á–∏–ª—Å—è: {client_id}")
    
    frame_samples = int(SAMPLE_RATE * VADConfig.FRAME_MS / 1000)
    pcm_buffer = np.array([], dtype=np.float32)
    
    # –°—á—ë—Ç—á–∏–∫ —Ç–∏—à–∏–Ω—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ
    silence_streak = 0
    MAX_SILENCE_BEFORE_SKIP = 50  # ~1.5 —Å–µ–∫ —Ç–∏—à–∏–Ω—ã –ø–æ–¥—Ä—è–¥ = —Å–∫–∏–ø–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
    
    try:
        await websocket.send(json.dumps({
            "type": "connected",
            "message": "ChatGPT-style STT server ready (v2.1 anti-hallucination)",
            "sample_rate": SAMPLE_RATE,
            "model": "whisper-small",
            "device": device,
            "features": [
                "adaptive_pause_detection",
                "streaming_partials",
                "speaker_identification",
                "hallucination_filter",
            ],
        }))
        
        async for message in websocket:
            try:
                data = json.loads(message)
                msg_type = data.get("type")
                
                if msg_type == "audio":
                    audio_b64 = data.get("audio") or ""
                    if not audio_b64:
                        continue
                    
                    audio_chunk = np.frombuffer(
                        base64.b64decode(audio_b64),
                        dtype=np.int16
                    ).astype(np.float32) / 32768.0
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —ç–Ω–µ—Ä–≥–∏—é —á–∞–Ω–∫–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç–∏—à–∏–Ω—ã
                    chunk_energy = calculate_energy(audio_chunk)
                    if chunk_energy < VADConfig.ENERGY_THRESHOLD:
                        silence_streak += 1
                    else:
                        silence_streak = 0
                    
                    # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ —Ç–∏—à–∏–Ω–∞ - —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
                    if silence_streak > MAX_SILENCE_BEFORE_SKIP:
                        if session.state != SpeechState.SILENCE:
                            print(f"üîá [{client_id}] Long silence detected, resetting buffers")
                            session.speech_buffer = []
                            session.state = SpeechState.SILENCE
                            session.speech_frames = 0
                            session.silence_frames = 0
                        pcm_buffer = np.array([], dtype=np.float32)
                        continue
                    
                    pcm_buffer = np.concatenate([pcm_buffer, audio_chunk])
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ—Ä–µ–π–º—ã
                    while len(pcm_buffer) >= frame_samples:
                        frame = pcm_buffer[:frame_samples]
                        pcm_buffer = pcm_buffer[frame_samples:]
                        
                        result = await process_vad_frame(session, frame, websocket)
                        if result:
                            await websocket.send(json.dumps(result))
                
                elif msg_type == "finalize":
                    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
                    if session.state != SpeechState.SILENCE:
                        result = await finalize_segment(session)
                        if result:
                            await websocket.send(json.dumps(result))
                        else:
                            await websocket.send(json.dumps({
                                "type": "transcription",
                                "text": "",
                                "is_final": True,
                                "timestamp": datetime.now().isoformat(),
                            }))
                    else:
                        if len(pcm_buffer) > frame_samples:
                            session.speech_buffer = [pcm_buffer]
                            result = await finalize_segment(session)
                            pcm_buffer = np.array([], dtype=np.float32)
                            if result:
                                await websocket.send(json.dumps(result))
                            else:
                                await websocket.send(json.dumps({
                                    "type": "transcription",
                                    "text": "",
                                    "is_final": True,
                                    "timestamp": datetime.now().isoformat(),
                                }))
                        else:
                            await websocket.send(json.dumps({
                                "type": "transcription",
                                "text": "",
                                "is_final": True,
                                "timestamp": datetime.now().isoformat(),
                            }))
                
                elif msg_type == "ping":
                    await websocket.send(json.dumps({
                        "type": "pong",
                        "timestamp": datetime.now().isoformat(),
                    }))
                
                elif msg_type == "reset":
                    session.conversation_context = []
                    session.last_transcript = ""
                    await websocket.send(json.dumps({
                        "type": "reset_ack",
                        "timestamp": datetime.now().isoformat(),
                    }))
                
                elif msg_type == "config":
                    # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                    if "pause_ms" in data:
                        VADConfig.DEFAULT_PAUSE_MS = int(data["pause_ms"])
                    if "energy_threshold" in data:
                        VADConfig.ENERGY_THRESHOLD = float(data["energy_threshold"])
                    await websocket.send(json.dumps({
                        "type": "config_ack",
                        "config": {
                            "pause_ms": VADConfig.DEFAULT_PAUSE_MS,
                            "energy_threshold": VADConfig.ENERGY_THRESHOLD,
                        },
                    }))
            
            except json.JSONDecodeError:
                await websocket.send(json.dumps({
                    "type": "error",
                    "message": "Invalid JSON",
                }))
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                import traceback
                traceback.print_exc()
                await websocket.send(json.dumps({
                    "type": "error",
                    "message": str(e),
                }))
    
    except websockets.exceptions.ConnectionClosed:
        print(f"üîå –ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è: {client_id}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ handle_client: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print(f"üëã –°–µ—Å—Å–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {client_id}")
        print(f"   üìä –í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {session.total_segments}")
        print(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è —Ä–µ—á–∏: {session.total_speech_ms/1000:.1f}—Å")
        print(f"   üé≠ –°–ø–∏–∫–µ—Ä–æ–≤: {session.speaker_counter}")
        
        with sessions_lock:
            if client_id in sessions:
                del sessions[client_id]


async def main():
    """–ó–∞–ø—É—Å–∫ WebSocket —Å–µ—Ä–≤–µ—Ä–∞"""
    server = await websockets.serve(
        handle_client,
        "0.0.0.0",
        8765,
        ping_interval=20,
        ping_timeout=20,
        max_size=10 * 1024 * 1024  # 10MB max message size
    )
    
    print("üéß –û–∂–∏–¥–∞—é –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π...")
    await server.wait_closed()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nüëã –°–µ—Ä–≤–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
